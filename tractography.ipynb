{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nrrd\n",
    "import nibabel as nb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dipy.tracking.local import LocalTracking, ThresholdTissueClassifier\n",
    "from dipy.tracking.utils import random_seeds_from_mask\n",
    "from dipy.reconst.dti import TensorModel\n",
    "from dipy.reconst.csdeconv import (ConstrainedSphericalDeconvModel,\n",
    "                                   auto_response)\n",
    "from dipy.reconst.shm import CsaOdfModel\n",
    "from dipy.data import default_sphere\n",
    "from dipy.direction import peaks_from_model\n",
    "from dipy.data import fetch_stanford_hardi, read_stanford_hardi, get_sphere\n",
    "from dipy.segment.mask import median_otsu\n",
    "from dipy.viz import actor, window\n",
    "from dipy.io.image import save_nifti\n",
    "from dipy.io import read_bvals_bvecs\n",
    "from dipy.core import gradients\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "from dipy.io import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table, gradient_table_from_bvals_bvecs\n",
    "from dipy.reconst.dti import fractional_anisotropy\n",
    "\n",
    "from dipy.tracking import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pTrainData = 'train_grid.h5'\n",
    "pTrainInput = 'train_input_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** visualize streamlines of specific anatomic regions such as corpus callosum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.local import LocalTracking\n",
    "from dipy.viz import window, actor\n",
    "from dipy.viz.colormap import line_colors\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "from dipy.tracking import metrics\n",
    "\n",
    "\n",
    "def visSphere(sphere):\n",
    "    '''\n",
    "    Visualize sphere\n",
    "    '''\n",
    "    ren = window.Renderer()\n",
    "    ren.SetBackground(1, 1, 1)\n",
    "    ren.add(actor.point(sphere.vertices, window.colors.red, point_radius=0.05))\n",
    "    window.show(ren)\n",
    "\n",
    "def visStreamlines(streamlines, volume, vol_slice_idx = 40):\n",
    "    '''\n",
    "    Visualize streamline using vtk\n",
    "    '''\n",
    "    # Prepare the display objects.\n",
    "    color = line_colors(streamlines)\n",
    "\n",
    "    if window.have_vtk:\n",
    "        vol_actor = actor.slicer(volume)\n",
    "\n",
    "        vol_actor.display(x=vol_slice_idx)\n",
    "        vol_actor2 = vol_actor.copy()\n",
    "        vol_actor2.display(z=35)\n",
    "        \n",
    "        streamlines_actor = actor.line(streamlines, line_colors(streamlines))\n",
    "\n",
    "        # Create the 3D display.\n",
    "        r = window.Renderer()\n",
    "        r.add(streamlines_actor)\n",
    "        r.add(vol_actor)\n",
    "        window.record(r, n_frames=1, out_path='deterministic.png', size=(800, 800))\n",
    "        window.show(r)\n",
    "    else:\n",
    "        print('we need VTK for proper visualisation of our fibres.')\n",
    "        \n",
    "def filterStreamlinesByLength(streamlines, minimumLength = 80):\n",
    "    '''\n",
    "    Removes streamlines that are shorter (in mm) than minimumLength\n",
    "    '''\n",
    "    return [x for x in streamlines if metrics.length(x) > minimumLength]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load bvec/bval and compute gradient table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvals, bvecs = read_bvals_bvecs('100307/bvals', '100307/bvecs')\n",
    "gtab = gradient_table(bvals=bvals, bvecs=bvecs,b0_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-values shape (288,)\n",
      "         min 5.000000 \n",
      "         max 3010.000000 \n",
      "B-vectors shape (288, 3)\n",
      "         min -0.999389 \n",
      "         max 0.999012 \n"
     ]
    }
   ],
   "source": [
    "gtab.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(gtab.bvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nb.load('100307/data.nii.gz')\n",
    "dwi = img.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate isotropic voxel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.25, 1.25, 1.25)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.header.get_zooms()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 174, 145, 288)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load binary brain segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarymask, options = nrrd.read('100307/nodif_brain_mask.nrrd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize DWI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADZCAYAAAAuX/tkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "axial_middle = dwi.shape[2] // 2\n",
    "plt.figure('Showing the datasets')\n",
    "plt.subplot(1, 2, 1).set_axis_off()\n",
    "plt.imshow(dwi[:, :, axial_middle, 0].T, cmap='gray', origin='lower')\n",
    "plt.subplot(1, 2, 2).set_axis_off()\n",
    "plt.imshow(dwi[:, :, axial_middle, 10].T, cmap='gray', origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tissue Segmentation\n",
    "**TODO**: check if datasets are registered\n",
    "\n",
    "**TODO**: not working right yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation of T1w data wrt. tissue state (cortico spinal fluid, gray- & white matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nb.load('100307/T1w_acpc_dc_restore_1.25.nii.gz')\n",
    "t1 = img.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.segment.tissue import TissueClassifierHMRF\n",
    "nclass = 3\n",
    "beta = 0.1 # smoothness regularizer\n",
    "hmrf = TissueClassifierHMRF()\n",
    "initial_segmentation, final_segmentation, PVE = hmrf.classify(t1, nclass, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "a = fig.add_subplot(1, 3, 1)\n",
    "img_ax = np.rot90(PVE[..., 89, 0])\n",
    "imgplot = plt.imshow(img_ax, cmap=\"gray\")\n",
    "a.axis('off')\n",
    "a.set_title('CSF')\n",
    "a = fig.add_subplot(1, 3, 2)\n",
    "img_cor = np.rot90(PVE[:, :, 89, 1])\n",
    "imgplot = plt.imshow(img_cor, cmap=\"gray\")\n",
    "a.axis('off')\n",
    "a.set_title('Gray Matter')\n",
    "a = fig.add_subplot(1, 3, 3)\n",
    "img_cor = np.rot90(PVE[:, :, 89, 2])\n",
    "imgplot = plt.imshow(img_cor, cmap=\"gray\")\n",
    "a.axis('off')\n",
    "a.set_title('White Matter')\n",
    "plt.savefig('probabilities.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tractography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll generate streamlines using different approaches. This is gonna be the foundation for the evaluation of our method. We'll also employ simulated as well as curated data for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.utils import random_seeds_from_mask, seeds_from_mask\n",
    "ccmask = np.zeros(binarymask.shape)\n",
    "ccmask[20:50,55:85,38:39] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccseeds = seeds_from_mask(ccmask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Tensor Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute fractional anisotropy and select seeds_count seeds per voxel with FA > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dipy.reconst.dti as dti\n",
    "dti_wls = dti.TensorModel(gtab)\n",
    "# roi_idx = (slice(20, 50), slice(55, 85), slice(38, 39)) #  splenium of the corpus callosum\n",
    "fit_wls = dti_wls.fit(dwi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate seeds at each voxel for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA = fit_wls.fa\n",
    "seeds = random_seeds_from_mask(FA > 0.5, seeds_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere = get_sphere('symmetric724')\n",
    "start_time = time.time()\n",
    "dtipeaks = peaks_from_model(model=dti_wls,\n",
    "                            data=dwi,\n",
    "                            sphere=sphere,\n",
    "                            relative_peak_threshold=.5,\n",
    "                            min_separation_angle=25,\n",
    "                            mask=binarymask,\n",
    "                            return_odf=False,\n",
    "                            parallel=True,\n",
    "                            normalize_peaks=False)\n",
    "runtime = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFA = dtipeaks.gfa\n",
    "print('Runtime ' + str(runtime) + 's / GFA.shape (%d, %d, %d)' % GFA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ThresholdTissueClassifier(dtipeaks.gfa, .25)\n",
    "streamlines_generator = LocalTracking(dtipeaks, classifier, seeds, np.identity(4), step_size=.5)\n",
    "streamlines = Streamlines(streamlines_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visStreamlines(streamlines,t1w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_filtered = filterStreamlinesByLength(streamlines, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\dti\\lib\\site-packages\\vtk\\util\\numpy_support.py:137: FutureWarning: Conversion of the second argument of issubdtype from `complex` to `np.complexfloating` is deprecated. In future, it will be treated as `np.complex128 == np.dtype(complex).type`.\n",
      "  assert not numpy.issubdtype(z.dtype, complex), \\\n"
     ]
    }
   ],
   "source": [
    "visStreamlines(streamlines_filtered,t1,vol_slice_idx=76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamline playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import values_from_volume\n",
    "#from dipy.tracking.utils import length\n",
    "import dipy.align.vector_fields as vfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = values_from_volume(t1,streamlines_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfa = np.asarray(streamlines_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfa[0][0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwi[23,95,47,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfa[0][0:5,:]\n",
    "vfu.interpolate_scalar_3d(dwi[:,:,:,0],sfa[0][0:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.data import get_sphere\n",
    "sphere = get_sphere('symmetric724')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-ball Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csamodel = CsaOdfModel(gtab, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere = get_sphere('symmetric724')\n",
    "start_time = time.time()\n",
    "csapeaks = peaks_from_model(model=csamodel,\n",
    "                            data=dwi,\n",
    "                            sphere=sphere,\n",
    "                            relative_peak_threshold=.5,\n",
    "                            min_separation_angle=25,\n",
    "                            mask=binarymask,\n",
    "                            return_odf=False,\n",
    "                            parallel=True,\n",
    "                            normalize_peaks=False)\n",
    "\n",
    "GFA = csapeaks.gfa\n",
    "runtime = time.time() - start_time\n",
    "print('Runtime ' + str(runtime) + ' s / GFA.shape (%d, %d, %d)' % GFA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ThresholdTissueClassifier(csapeaks.gfa, .25)\n",
    "streamlines_generator = LocalTracking(csapeaks, classifier, seeds, np.identity(4), step_size=.5)\n",
    "streamlines = Streamlines(streamlines_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_filtered = filterStreamlinesByLength(streamlines, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(streamlines_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visStreamlines(streamlines,t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Spherical Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, ratio = auto_response(gtab, dwi, roi_radius=10, fa_thr=0.5)\n",
    "print(response)\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_model = ConstrainedSphericalDeconvModel(gtab, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere = get_sphere('symmetric724')\n",
    "start_time = time.time()\n",
    "csd_peaks = peaks_from_model(model=csd_model,\n",
    "                             data=dwi,\n",
    "                             sphere=sphere,\n",
    "                             mask=binarymask,\n",
    "                             relative_peak_threshold=.5,\n",
    "                             min_separation_angle=25,\n",
    "                             parallel=True)\n",
    "GFA = csapeaks.gfa\n",
    "runtime = time.time() - start_time\n",
    "print('Runtime ' + str(runtime) + ' s / GFA.shape (%d, %d, %d)' % GFA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ThresholdTissueClassifier(csd_peaks.gfa, .25)\n",
    "streamlines_generator = LocalTracking(csd_peaks, classifier, seeds, np.identity(4), step_size=.5)\n",
    "streamlines = Streamlines(streamlines_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visStreamlines(streamlines,t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomically curated white matter atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** load atlas ORG-800FC-100HCP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** apply atlas based tractography approach of Lauren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.utils import length # compute length of each streamline in mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics: average lengths, touches gray matter, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_filtered, streamlines, seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(streamlines_filtered).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(pTrainInput + \"_sl_filt.npy\",streamlines_filtered)\n",
    "np.save(pTrainInput + \"_sl.npy\",streamlines)\n",
    "np.save(pTrainInput + \"_seeds.npy\",seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_filtered = np.load(pTrainInput + \"_sl_filt.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_filtered = np.asarray(streamlines_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(streamlines_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: (current Position, last direction, local DWI data) -> new direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** generate synthetic training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1st simple training data generator **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates training data given segmented streamlines and a dwi dataset. The DWI data of a spheroidal region D_{N_{p_i}} next to the current streamline position p_i will be used to predict p_{i+1}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** reshape X such that it reflects the spatial orientation of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Parallelize + optimize data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import values_from_volume\n",
    "import dipy.align.vector_fields as vfu\n",
    "from dipy.core.sphere import Sphere\n",
    "from dipy.core import subdivide_octahedron \n",
    "\n",
    "def generateSimpleTraindataFromStreamlines(streamlines, dwi, rec_level_sphere = 3):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    sfa = np.asarray(streamlines)\n",
    "    sph = subdivide_octahedron.create_unit_sphere(recursion_level=rec_level_sphere) # create unit sphere with 4 ** recursion_level + 2 vertices\n",
    "    #visSphere(sph)\n",
    "    dx,dy,dz,dw = dwi.shape\n",
    "    noStreamlines = min(len(sfa), 1000) # FIXME: spares us some time right now\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    \n",
    "    for streamlineIndex in range(0,noStreamlines):\n",
    "        print('Streamline ' + str(streamlineIndex) + '/' + str(noStreamlines))\n",
    "        lengthStreamline = len(sfa[streamlineIndex])\n",
    "        for streamlineElementIndex in range(0,lengthStreamline-1):\n",
    "            # center sphere around current streamline position\n",
    "            sph2 = sph.vertices + sfa[streamlineIndex][streamlineElementIndex]\n",
    "            sph2 = np.vstack((sph2,sfa[streamlineIndex][streamlineElementIndex]))\n",
    "            \n",
    "            # interpolate data given these coordinates for each channel\n",
    "            x = np.zeros([4**rec_level_sphere+2+1,dw])\n",
    "            for i in range(0,dw):\n",
    "                x[:,i] = vfu.interpolate_scalar_3d(dwi[:,:,:,i],sph2)[0]\n",
    "            train_X.append(x)\n",
    "            \n",
    "            old_y = sfa[streamlineIndex][streamlineElementIndex+1]\n",
    "            nextStreamlineDirection = sfa[streamlineIndex][streamlineElementIndex] - sfa[streamlineIndex][streamlineElementIndex+1]\n",
    "            \n",
    "            train_Y.append(nextStreamlineDirection) # dont store absolute value but relative displacement\n",
    "    train_X = np.asarray(train_X)\n",
    "    train_Y = np.asarray(train_Y)\n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_X, train_Y = generateSimpleTraindataFromStreamlines(streamlines_filtered, dwi)\n",
    "runtime = time.time() - start_time\n",
    "print('Runtime ' + str(runtime) + ' s / GFA.shape (%d, %d, %d)' % GFA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import values_from_volume\n",
    "import dipy.align.vector_fields as vfu\n",
    "from dipy.core.sphere import Sphere\n",
    "from dipy.core import subdivide_octahedron \n",
    "\n",
    "def generateGridSimpleTraindataFromStreamlines(streamlines, dwi, rec_level_sphere = 3, noX=3, noY=3,noZ=3,coordinateScaling = 1):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    sfa = np.asarray(streamlines)\n",
    "    dx,dy,dz,dw = dwi.shape\n",
    "    noStreamlines = min(len(sfa), 1000) # FIXME: spares us some time right now\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    \n",
    "    for streamlineIndex in range(0,noStreamlines):\n",
    "        if((streamlineIndex % 100) == 0):\n",
    "            print('Streamline ' + str(streamlineIndex) + '/' + str(noStreamlines))\n",
    "        lengthStreamline = int(len(sfa[streamlineIndex]) / 10)\n",
    "        for streamlineElementIndex in range(0,lengthStreamline-1):           \n",
    "            x_ = coordinateScaling * np.linspace(-1., 1., noX)\n",
    "            y_ = coordinateScaling * np.linspace(-1., 1., noY)\n",
    "            z_ = coordinateScaling * np.linspace(-1., 1., noZ)\n",
    "            coordVecs = np.vstack(np.meshgrid(x_,y_,z_)).reshape(3,-1).T + sfa[streamlineIndex][streamlineElementIndex]\n",
    "            \n",
    "            # interpolate data given these coordinates for each channel\n",
    "            x = np.zeros([noX,noY,noZ,dw])\n",
    "            for i in range(0,dw):\n",
    "                x[:,:,:,i] = np.reshape(vfu.interpolate_scalar_3d(dwi[:,:,:,i],coordVecs)[0], [noX,noY,noZ])\n",
    "            train_X.append(x)\n",
    "            old_y = sfa[streamlineIndex][streamlineElementIndex+1]\n",
    "            nextStreamlineDirection = sfa[streamlineIndex][streamlineElementIndex] - sfa[streamlineIndex][streamlineElementIndex+1]\n",
    "            \n",
    "            train_Y.append(nextStreamlineDirection) # dont store absolute value but relative displacement\n",
    "    train_X = np.asarray(train_X)\n",
    "    train_Y = np.asarray(train_Y)\n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamline 0/1000\n",
      "Streamline 100/1000\n",
      "Streamline 200/1000\n",
      "Streamline 300/1000\n",
      "Streamline 400/1000\n",
      "Streamline 500/1000\n",
      "Streamline 600/1000\n",
      "Streamline 700/1000\n",
      "Streamline 800/1000\n",
      "Streamline 900/1000\n",
      "Runtime 496.7569999694824 s \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_X_16_16_16, train_Y_16_16_16 = generateGridSimpleTraindataFromStreamlines(streamlines_filtered, dwi, noX=8,noY=8,noZ=8,coordinateScaling=0.5)\n",
    "runtime = time.time() - start_time\n",
    "print('Runtime ' + str(runtime) + ' s ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15145, 8, 8, 8, 288)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_16_16_16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15145, 3)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_16_16_16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(pTrainData+\"1k_8_8_8_cs0.5\",\"w\") as f:\n",
    "    d1 = f.create_dataset('train_X',data=train_X_16_16_16)\n",
    "    d2 = f.create_dataset('train_Y',data=train_Y_16_16_16)\n",
    "#    d3 = f.create_dataset('streamlines_filtered',data=streamlines_filtered)\n",
    "#    d4 = f.create_dataset('dwi',data=dwi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X_888\n",
    "train_Y = train_Y_888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2nd training data generator **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X the same as before, however predict sphere. each face of the sphere (direction) represents the distance/probability of a streamline in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import values_from_volume\n",
    "import dipy.align.vector_fields as vfu\n",
    "from dipy.core.sphere import Sphere\n",
    "from dipy.core import subdivide_octahedron \n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "def generateGridTraindataFromStreamlines(streamlines, dwi, rec_level_sphere = 3, noX=3, noY=3,noZ=3,coordinateScaling = 1, noCrossings = 3):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    sfa = np.asarray(streamlines)\n",
    "    np.random.shuffle(sfa)\n",
    "    dx,dy,dz,dw = dwi.shape\n",
    "    noStreamlines = min(len(sfa), 1000) # FIXME: spares us some time right now\n",
    "    noNeighbours = 2*noCrossings + 1\n",
    "    sl_pos = sfa[0]\n",
    "    print('Building kd-tree of streamline positions')\n",
    "    for streamlineIndex in range(1,noStreamlines):\n",
    "        lengthStreamline = int(len(sfa[streamlineIndex]) / 10)\n",
    "        sl_pos = np.concatenate([sl_pos, sfa[streamlineIndex][0:lengthStreamline]], axis=0) # dont store absolute value but relative displacement\n",
    "    \n",
    "    kdt = KDTree(sl_pos)\n",
    "    \n",
    "    print('Building actual training data')\n",
    "    #train_X = []\n",
    "    #train_Y = []\n",
    "    ctr = 0\n",
    "    \n",
    "    x_ = coordinateScaling * np.linspace(-1., 1., noX)\n",
    "    y_ = coordinateScaling * np.linspace(-1., 1., noY)\n",
    "    z_ = coordinateScaling * np.linspace(-1., 1., noZ)\n",
    "    train_Y = np.zeros([len(sl_pos),2*noCrossings,3])\n",
    "    train_X = np.zeros([len(sl_pos),noX,noY,noZ,dw])\n",
    "    for streamlinevec in sl_pos:\n",
    "        d,i = kdt.query(streamlinevec,noNeighbours)\n",
    "        i = i[1:]\n",
    "        n_slv = streamlinevec - kdt.data[i]\n",
    "        #train_Y.append(n_slv)\n",
    "        train_Y[ctr,:,:] = n_slv\n",
    "        if((ctr % 50) == 0):\n",
    "            print(str(ctr) + \"/\" + str(len(sl_pos)))\n",
    "        coordVecs = np.vstack(np.meshgrid(x_,y_,z_)).reshape(3,-1).T + streamlinevec\n",
    "            \n",
    "        # interpolate data given these coordinates for each channel\n",
    "        #x = np.zeros([noX,noY,noZ,dw])\n",
    "        for i in range(0,dw):\n",
    "            #x[:,:,:,i] = np.reshape(vfu.interpolate_scalar_3d(dwi[:,:,:,i],coordVecs)[0], [noX,noY,noZ])\n",
    "            train_X[ctr,:,:,:,i] = np.reshape(vfu.interpolate_scalar_3d(dwi[:,:,:,i],coordVecs)[0], [noX,noY,noZ])\n",
    "        #train_X.append(x)\n",
    "        ctr += 1\n",
    "        \n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building kd-tree of streamline positions\n",
      "Building actual training data\n",
      "0/16153\n",
      "50/16153\n",
      "100/16153\n",
      "150/16153\n",
      "200/16153\n",
      "250/16153\n",
      "300/16153\n",
      "350/16153\n",
      "400/16153\n",
      "450/16153\n",
      "500/16153\n",
      "550/16153\n",
      "600/16153\n",
      "650/16153\n",
      "700/16153\n",
      "750/16153\n",
      "800/16153\n",
      "850/16153\n",
      "900/16153\n",
      "950/16153\n",
      "1000/16153\n",
      "1050/16153\n",
      "1100/16153\n",
      "1150/16153\n",
      "1200/16153\n",
      "1250/16153\n",
      "1300/16153\n",
      "1350/16153\n",
      "1400/16153\n",
      "1450/16153\n",
      "1500/16153\n",
      "1550/16153\n",
      "1600/16153\n",
      "1650/16153\n",
      "1700/16153\n",
      "1750/16153\n",
      "1800/16153\n",
      "1850/16153\n",
      "1900/16153\n",
      "1950/16153\n",
      "2000/16153\n",
      "2050/16153\n",
      "2100/16153\n",
      "2150/16153\n",
      "2200/16153\n",
      "2250/16153\n",
      "2300/16153\n",
      "2350/16153\n",
      "2400/16153\n",
      "2450/16153\n",
      "2500/16153\n",
      "2550/16153\n",
      "2600/16153\n",
      "2650/16153\n",
      "2700/16153\n",
      "2750/16153\n",
      "2800/16153\n",
      "2850/16153\n",
      "2900/16153\n",
      "2950/16153\n",
      "3000/16153\n",
      "3050/16153\n",
      "3100/16153\n",
      "3150/16153\n",
      "3200/16153\n",
      "3250/16153\n",
      "3300/16153\n",
      "3350/16153\n",
      "3400/16153\n",
      "3450/16153\n",
      "3500/16153\n",
      "3550/16153\n",
      "3600/16153\n",
      "3650/16153\n",
      "3700/16153\n",
      "3750/16153\n",
      "3800/16153\n",
      "3850/16153\n",
      "3900/16153\n",
      "3950/16153\n",
      "4000/16153\n",
      "4050/16153\n",
      "4100/16153\n",
      "4150/16153\n",
      "4200/16153\n",
      "4250/16153\n",
      "4300/16153\n",
      "4350/16153\n",
      "4400/16153\n",
      "4450/16153\n",
      "4500/16153\n",
      "4550/16153\n",
      "4600/16153\n",
      "4650/16153\n",
      "4700/16153\n",
      "4750/16153\n",
      "4800/16153\n",
      "4850/16153\n",
      "4900/16153\n",
      "4950/16153\n",
      "5000/16153\n",
      "5050/16153\n",
      "5100/16153\n",
      "5150/16153\n",
      "5200/16153\n",
      "5250/16153\n",
      "5300/16153\n",
      "5350/16153\n",
      "5400/16153\n",
      "5450/16153\n",
      "5500/16153\n",
      "5550/16153\n",
      "5600/16153\n",
      "5650/16153\n",
      "5700/16153\n",
      "5750/16153\n",
      "5800/16153\n",
      "5850/16153\n",
      "5900/16153\n",
      "5950/16153\n",
      "6000/16153\n",
      "6050/16153\n",
      "6100/16153\n",
      "6150/16153\n",
      "6200/16153\n",
      "6250/16153\n",
      "6300/16153\n",
      "6350/16153\n",
      "6400/16153\n",
      "6450/16153\n",
      "6500/16153\n",
      "6550/16153\n",
      "6600/16153\n",
      "6650/16153\n",
      "6700/16153\n",
      "6750/16153\n",
      "6800/16153\n",
      "6850/16153\n",
      "6900/16153\n",
      "6950/16153\n",
      "7000/16153\n",
      "7050/16153\n",
      "7100/16153\n",
      "7150/16153\n",
      "7200/16153\n",
      "7250/16153\n",
      "7300/16153\n",
      "7350/16153\n",
      "7400/16153\n",
      "7450/16153\n",
      "7500/16153\n",
      "7550/16153\n",
      "7600/16153\n",
      "7650/16153\n",
      "7700/16153\n",
      "7750/16153\n",
      "7800/16153\n",
      "7850/16153\n",
      "7900/16153\n",
      "7950/16153\n",
      "8000/16153\n",
      "8050/16153\n",
      "8100/16153\n",
      "8150/16153\n",
      "8200/16153\n",
      "8250/16153\n",
      "8300/16153\n",
      "8350/16153\n",
      "8400/16153\n",
      "8450/16153\n",
      "8500/16153\n",
      "8550/16153\n",
      "8600/16153\n",
      "8650/16153\n",
      "8700/16153\n",
      "8750/16153\n",
      "8800/16153\n",
      "8850/16153\n",
      "8900/16153\n",
      "8950/16153\n",
      "9000/16153\n",
      "9050/16153\n",
      "9100/16153\n",
      "9150/16153\n",
      "9200/16153\n",
      "9250/16153\n",
      "9300/16153\n",
      "9350/16153\n",
      "9400/16153\n",
      "9450/16153\n",
      "9500/16153\n",
      "9550/16153\n",
      "9600/16153\n",
      "9650/16153\n",
      "9700/16153\n",
      "9750/16153\n",
      "9800/16153\n",
      "9850/16153\n",
      "9900/16153\n",
      "9950/16153\n",
      "10000/16153\n",
      "10050/16153\n",
      "10100/16153\n",
      "10150/16153\n",
      "10200/16153\n",
      "10250/16153\n",
      "10300/16153\n",
      "10350/16153\n",
      "10400/16153\n",
      "10450/16153\n",
      "10500/16153\n",
      "10550/16153\n",
      "10600/16153\n",
      "10650/16153\n",
      "10700/16153\n",
      "10750/16153\n",
      "10800/16153\n",
      "10850/16153\n",
      "10900/16153\n",
      "10950/16153\n",
      "11000/16153\n",
      "11050/16153\n",
      "11100/16153\n",
      "11150/16153\n",
      "11200/16153\n",
      "11250/16153\n",
      "11300/16153\n",
      "11350/16153\n",
      "11400/16153\n",
      "11450/16153\n",
      "11500/16153\n",
      "11550/16153\n",
      "11600/16153\n",
      "11650/16153\n",
      "11700/16153\n",
      "11750/16153\n",
      "11800/16153\n",
      "11850/16153\n",
      "11900/16153\n",
      "11950/16153\n",
      "12000/16153\n",
      "12050/16153\n",
      "12100/16153\n",
      "12150/16153\n",
      "12200/16153\n",
      "12250/16153\n",
      "12300/16153\n",
      "12350/16153\n",
      "12400/16153\n",
      "12450/16153\n",
      "12500/16153\n",
      "12550/16153\n",
      "12600/16153\n",
      "12650/16153\n",
      "12700/16153\n",
      "12750/16153\n",
      "12800/16153\n",
      "12850/16153\n",
      "12900/16153\n",
      "12950/16153\n",
      "13000/16153\n",
      "13050/16153\n",
      "13100/16153\n",
      "13150/16153\n",
      "13200/16153\n",
      "13250/16153\n",
      "13300/16153\n",
      "13350/16153\n",
      "13400/16153\n",
      "13450/16153\n",
      "13500/16153\n",
      "13550/16153\n",
      "13600/16153\n",
      "13650/16153\n",
      "13700/16153\n",
      "13750/16153\n",
      "13800/16153\n",
      "13850/16153\n",
      "13900/16153\n",
      "13950/16153\n",
      "14000/16153\n",
      "14050/16153\n",
      "14100/16153\n",
      "14150/16153\n",
      "14200/16153\n",
      "14250/16153\n",
      "14300/16153\n",
      "14350/16153\n",
      "14400/16153\n",
      "14450/16153\n",
      "14500/16153\n",
      "14550/16153\n",
      "14600/16153\n",
      "14650/16153\n",
      "14700/16153\n",
      "14750/16153\n",
      "14800/16153\n",
      "14850/16153\n",
      "14900/16153\n",
      "14950/16153\n",
      "15000/16153\n",
      "15050/16153\n",
      "15100/16153\n",
      "15150/16153\n",
      "15200/16153\n",
      "15250/16153\n",
      "15300/16153\n",
      "15350/16153\n",
      "15400/16153\n",
      "15450/16153\n",
      "15500/16153\n",
      "15550/16153\n",
      "15600/16153\n",
      "15650/16153\n",
      "15700/16153\n",
      "15750/16153\n",
      "15800/16153\n",
      "15850/16153\n",
      "15900/16153\n",
      "15950/16153\n",
      "16000/16153\n",
      "16050/16153\n",
      "16100/16153\n",
      "16150/16153\n",
      "Runtime 487.8899998664856 s \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X,Y = generateGridTraindataFromStreamlines(streamlines_filtered, dwi, noX=8,noY=8,noZ=8,coordinateScaling=0.1)\n",
    "runtime = time.time() - start_time\n",
    "print('Runtime ' + str(runtime) + ' s ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(pTrainData+\"16k_8_8_8_cs0.1_Y6x3\",\"w\") as f:\n",
    "    d1 = f.create_dataset('train_X',data=X)\n",
    "    d2 = f.create_dataset('train_Y',data=Y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16153"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building kd-tree of streamline positions\n",
      "Building actual training data\n",
      "0/389\n",
      "50/389\n",
      "100/389\n",
      "150/389\n",
      "200/389\n",
      "250/389\n",
      "300/389\n",
      "350/389\n",
      "Runtime 11.883000135421753 s \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X,Y = generateGridTraindataFromStreamlines(streamlines_filtered, dwi, noX=8,noY=8,noZ=8)\n",
    "runtime = time.time() - start_time\n",
    "print('Runtime ' + str(runtime) + ' s ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16153"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(934, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import nn_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pTrainData = 'train_grid.h5'\n",
    "pTrainInput = 'train_input_'\n",
    "f = h5py.File(pTrainData, \"r\")\n",
    "train_X = np.array(f[\"train_X\"].value)\n",
    "train_Y = np.array(f[\"train_Y\"].value)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_n = train_Y - np.min(train_Y)\n",
    "train_Y_n /= np.max(train_Y_n)\n",
    "train_X_n = train_X - np.min(train_X)\n",
    "train_X_n /= np.max(train_X_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**8\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_unet = nn_helper.get_3Dunet(inputShape=(8, 8, 8, 288), depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 8, 8, 8, 288) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 8, 8, 8, 64)  497728      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 8, 64)  256         conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       multiple             0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "                                                                 conv3d_18[0][0]                  \n",
      "                                                                 conv3d_19[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 8, 8, 8, 64)  110656      leaky_re_lu_2[10][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 8, 64)  256         conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)  (None, 4, 4, 4, 64)  0           leaky_re_lu_2[11][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 4, 4, 4, 64)  110656      max_pooling3d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 4, 64)  256         conv3d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 4, 4, 4, 64)  110656      leaky_re_lu_2[12][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 4, 64)  256         conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3D)  (None, 2, 2, 2, 64)  0           leaky_re_lu_2[13][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 2, 2, 2, 64)  110656      max_pooling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 2, 2, 2, 64)  256         conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)              (None, 2, 2, 2, 64)  110656      leaky_re_lu_2[14][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 2, 2, 2, 64)  256         conv3d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3D)  (None, 1, 1, 1, 64)  0           leaky_re_lu_2[15][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)              (None, 1, 1, 1, 64)  110656      max_pooling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_19 (Conv3D)              (None, 1, 1, 1, 64)  110656      leaky_re_lu_2[16][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_3 (UpSampling3D)  (None, 2, 2, 2, 64)  0           leaky_re_lu_2[17][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2, 2, 2, 128) 0           up_sampling3d_3[0][0]            \n",
      "                                                                 leaky_re_lu_2[15][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_20 (Conv3D)              (None, 2, 2, 2, 64)  221248      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 2, 2, 2, 64)  256         conv3d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_21 (Conv3D)              (None, 2, 2, 2, 64)  110656      leaky_re_lu_2[18][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 2, 2, 2, 64)  256         conv3d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_4 (UpSampling3D)  (None, 4, 4, 4, 64)  0           leaky_re_lu_2[19][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4, 4, 4, 128) 0           up_sampling3d_4[0][0]            \n",
      "                                                                 leaky_re_lu_2[13][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_22 (Conv3D)              (None, 4, 4, 4, 64)  221248      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 4, 4, 64)  256         conv3d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_23 (Conv3D)              (None, 4, 4, 4, 64)  110656      leaky_re_lu_2[20][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 4, 64)  256         conv3d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_5 (UpSampling3D)  (None, 8, 8, 8, 64)  0           leaky_re_lu_2[21][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 8, 128) 0           up_sampling3d_5[0][0]            \n",
      "                                                                 leaky_re_lu_2[11][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_24 (Conv3D)              (None, 8, 8, 8, 64)  221248      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 8, 64)  256         conv3d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_25 (Conv3D)              (None, 8, 8, 8, 64)  110656      leaky_re_lu_2[22][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 8, 64)  256         conv3d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_26 (Conv3D)              (None, 8, 8, 8, 3)   195         leaky_re_lu_2[23][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1536)         0           conv3d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "finalPrediction (Dense)         (None, 3)            4611        flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,275,910\n",
      "Trainable params: 2,274,374\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dense_unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12152 samples, validate on 3038 samples\n",
      "Epoch 1/100\n",
      " - 78s - loss: 1.0037 - val_loss: 1.5339\n",
      "Epoch 2/100\n",
      " - 70s - loss: 0.6588 - val_loss: 0.4576\n",
      "Epoch 3/100\n",
      " - 70s - loss: 0.5127 - val_loss: 0.3913\n",
      "Epoch 4/100\n",
      " - 70s - loss: 0.4440 - val_loss: 0.4561\n",
      "Epoch 5/100\n",
      " - 71s - loss: 0.3946 - val_loss: 0.2933\n",
      "Epoch 6/100\n",
      " - 70s - loss: 0.3624 - val_loss: 0.2766\n",
      "Epoch 7/100\n",
      " - 71s - loss: 0.3452 - val_loss: 1.6569\n",
      "Epoch 8/100\n",
      " - 71s - loss: 0.3253 - val_loss: 0.2490\n",
      "Epoch 9/100\n",
      " - 71s - loss: 0.3083 - val_loss: 0.2399\n",
      "Epoch 10/100\n",
      " - 70s - loss: 0.2826 - val_loss: 0.2358\n",
      "Epoch 11/100\n",
      " - 70s - loss: 0.2725 - val_loss: 0.2364\n",
      "Epoch 12/100\n",
      " - 70s - loss: 0.2621 - val_loss: 0.2342\n",
      "Epoch 13/100\n",
      " - 70s - loss: 0.2486 - val_loss: 0.2643\n",
      "Epoch 14/100\n",
      " - 70s - loss: 0.2429 - val_loss: 0.3526\n",
      "Epoch 15/100\n",
      " - 70s - loss: 0.2291 - val_loss: 0.2669\n",
      "Epoch 16/100\n",
      " - 71s - loss: 0.2106 - val_loss: 0.2212\n",
      "Epoch 17/100\n",
      " - 70s - loss: 0.2060 - val_loss: 0.1976\n",
      "Epoch 18/100\n",
      " - 70s - loss: 0.1949 - val_loss: 0.2153\n",
      "Epoch 19/100\n",
      " - 70s - loss: 0.1897 - val_loss: 0.2515\n",
      "Epoch 20/100\n",
      " - 70s - loss: 0.1863 - val_loss: 0.2260\n",
      "Epoch 21/100\n",
      " - 70s - loss: 0.1843 - val_loss: 0.2258\n",
      "Epoch 22/100\n",
      " - 70s - loss: 0.1718 - val_loss: 0.1963\n",
      "Epoch 23/100\n",
      " - 71s - loss: 0.1701 - val_loss: 0.3324\n",
      "Epoch 24/100\n",
      " - 70s - loss: 0.1668 - val_loss: 0.1878\n",
      "Epoch 25/100\n",
      " - 70s - loss: 0.1640 - val_loss: 0.2638\n",
      "Epoch 26/100\n",
      " - 71s - loss: 0.1640 - val_loss: 0.1662\n",
      "Epoch 27/100\n",
      " - 70s - loss: 0.1555 - val_loss: 0.1901\n",
      "Epoch 28/100\n",
      " - 71s - loss: 0.1518 - val_loss: 0.1962\n",
      "Epoch 29/100\n",
      " - 70s - loss: 0.1517 - val_loss: 0.2103\n",
      "Epoch 30/100\n",
      " - 70s - loss: 0.1498 - val_loss: 0.9527\n",
      "Epoch 31/100\n",
      " - 70s - loss: 0.1472 - val_loss: 0.1537\n",
      "Epoch 32/100\n",
      " - 70s - loss: 0.1410 - val_loss: 0.1802\n",
      "Epoch 33/100\n",
      " - 70s - loss: 0.1404 - val_loss: 0.1735\n",
      "Epoch 34/100\n",
      " - 70s - loss: 0.1405 - val_loss: 0.1539\n",
      "Epoch 35/100\n",
      " - 69s - loss: 0.1353 - val_loss: 0.2027\n",
      "Epoch 36/100\n",
      " - 69s - loss: 0.1438 - val_loss: 0.1584\n",
      "Epoch 37/100\n",
      " - 68s - loss: 0.1534 - val_loss: 0.6853\n",
      "Epoch 38/100\n",
      " - 68s - loss: 0.1493 - val_loss: 0.2330\n",
      "Epoch 39/100\n",
      " - 68s - loss: 0.1372 - val_loss: 0.1569\n",
      "Epoch 40/100\n",
      " - 71s - loss: 0.1315 - val_loss: 0.2678\n",
      "Epoch 41/100\n",
      " - 71s - loss: 0.1347 - val_loss: 0.4415\n",
      "Epoch 42/100\n",
      " - 70s - loss: 0.1258 - val_loss: 0.1821\n",
      "Epoch 43/100\n",
      " - 69s - loss: 0.1262 - val_loss: 0.2035\n",
      "Epoch 44/100\n",
      " - 69s - loss: 0.1206 - val_loss: 0.1817\n",
      "Epoch 45/100\n",
      " - 70s - loss: 0.1635 - val_loss: 0.5348\n",
      "Epoch 46/100\n",
      " - 69s - loss: 0.1420 - val_loss: 0.2361\n",
      "Epoch 47/100\n",
      " - 70s - loss: 0.1306 - val_loss: 0.1566\n",
      "Epoch 48/100\n",
      " - 69s - loss: 0.1222 - val_loss: 0.1882\n",
      "Epoch 49/100\n",
      " - 70s - loss: 0.1197 - val_loss: 0.1692\n",
      "Epoch 50/100\n",
      " - 70s - loss: 0.1168 - val_loss: 0.1647\n",
      "Epoch 51/100\n",
      " - 70s - loss: 0.1148 - val_loss: 0.1852\n",
      "Epoch 52/100\n",
      " - 69s - loss: 0.1109 - val_loss: 0.1619\n",
      "Epoch 53/100\n",
      " - 68s - loss: 0.1098 - val_loss: 0.1674\n",
      "Epoch 54/100\n",
      " - 69s - loss: 0.1079 - val_loss: 0.1643\n",
      "Epoch 55/100\n",
      " - 68s - loss: 0.1055 - val_loss: 0.1887\n",
      "Epoch 56/100\n",
      " - 68s - loss: 0.1051 - val_loss: 0.1940\n",
      "Epoch 57/100\n",
      " - 68s - loss: 0.1042 - val_loss: 0.1582\n",
      "Epoch 58/100\n",
      " - 68s - loss: 0.1060 - val_loss: 0.1955\n",
      "Epoch 59/100\n",
      " - 68s - loss: 0.1010 - val_loss: 0.1380\n",
      "Epoch 60/100\n",
      " - 68s - loss: 0.0989 - val_loss: 0.2699\n",
      "Epoch 61/100\n",
      " - 68s - loss: 0.0968 - val_loss: 0.1816\n",
      "Epoch 62/100\n",
      " - 68s - loss: 0.0932 - val_loss: 0.1438\n",
      "Epoch 63/100\n",
      " - 68s - loss: 0.0920 - val_loss: 0.1660\n",
      "Epoch 64/100\n",
      " - 68s - loss: 0.0897 - val_loss: 0.1450\n",
      "Epoch 65/100\n",
      " - 68s - loss: 0.0879 - val_loss: 0.1374\n",
      "Epoch 66/100\n",
      " - 68s - loss: 0.0869 - val_loss: 0.1339\n",
      "Epoch 67/100\n",
      " - 68s - loss: 0.0852 - val_loss: 0.1493\n",
      "Epoch 68/100\n",
      " - 68s - loss: 0.0864 - val_loss: 0.1511\n",
      "Epoch 69/100\n",
      " - 68s - loss: 0.0862 - val_loss: 0.1533\n",
      "Epoch 70/100\n",
      " - 69s - loss: 0.0822 - val_loss: 0.1353\n",
      "Epoch 71/100\n",
      " - 68s - loss: 0.0815 - val_loss: 0.1260\n",
      "Epoch 72/100\n",
      " - 68s - loss: 0.0801 - val_loss: 0.1696\n",
      "Epoch 73/100\n",
      " - 68s - loss: 0.0803 - val_loss: 0.1529\n",
      "Epoch 74/100\n",
      " - 68s - loss: 0.0795 - val_loss: 0.1304\n",
      "Epoch 75/100\n",
      " - 69s - loss: 0.0771 - val_loss: 0.1821\n",
      "Epoch 76/100\n",
      " - 68s - loss: 0.0805 - val_loss: 0.3111\n",
      "Epoch 77/100\n",
      " - 68s - loss: 0.0772 - val_loss: 0.1477\n",
      "Epoch 78/100\n",
      " - 68s - loss: 0.0751 - val_loss: 0.1384\n",
      "Epoch 79/100\n",
      " - 68s - loss: 0.0745 - val_loss: 0.1201\n",
      "Epoch 80/100\n",
      " - 68s - loss: 0.0707 - val_loss: 0.1183\n",
      "Epoch 81/100\n",
      " - 67s - loss: 0.0729 - val_loss: 0.2014\n",
      "Epoch 82/100\n",
      " - 67s - loss: 0.0711 - val_loss: 0.1634\n",
      "Epoch 83/100\n",
      " - 67s - loss: 0.0689 - val_loss: 0.1139\n",
      "Epoch 84/100\n",
      " - 68s - loss: 0.0688 - val_loss: 0.1230\n",
      "Epoch 85/100\n",
      " - 68s - loss: 0.0686 - val_loss: 0.1546\n",
      "Epoch 86/100\n",
      " - 68s - loss: 0.0673 - val_loss: 0.2108\n",
      "Epoch 87/100\n",
      " - 67s - loss: 0.0681 - val_loss: 0.1438\n",
      "Epoch 88/100\n",
      " - 67s - loss: 0.0666 - val_loss: 0.1174\n",
      "Epoch 89/100\n",
      " - 67s - loss: 0.0637 - val_loss: 0.1574\n",
      "Epoch 90/100\n",
      " - 67s - loss: 0.0707 - val_loss: 0.2136\n",
      "Epoch 91/100\n",
      " - 67s - loss: 0.0639 - val_loss: 0.1189\n",
      "Epoch 92/100\n",
      " - 67s - loss: 0.0656 - val_loss: 0.1171\n",
      "Epoch 93/100\n",
      " - 67s - loss: 0.0640 - val_loss: 0.1876\n",
      "Epoch 94/100\n",
      " - 67s - loss: 0.0620 - val_loss: 0.1869\n",
      "Epoch 95/100\n",
      " - 67s - loss: 0.0618 - val_loss: 0.1813\n",
      "Epoch 96/100\n",
      " - 67s - loss: 0.0628 - val_loss: 0.1477\n",
      "Epoch 97/100\n",
      " - 67s - loss: 0.0605 - val_loss: 0.1339\n",
      "Epoch 98/100\n",
      " - 67s - loss: 0.0611 - val_loss: 0.1242\n",
      "Epoch 99/100\n",
      " - 68s - loss: 0.0593 - val_loss: 0.1521\n",
      "Epoch 100/100\n",
      " - 69s - loss: 0.0597 - val_loss: 0.2535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x65088b00>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_unet.fit(train_X_n, train_Y_n, batch_size=batch_size, epochs=epochs,verbose=2,validation_split=0.2) # directional vectors, with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12152 samples, validate on 3038 samples\n",
      "Epoch 1/1000\n",
      " - 63s - loss: 0.0208 - val_loss: 0.0780\n",
      "Epoch 2/1000\n",
      " - 61s - loss: 0.0186 - val_loss: 0.0800\n",
      "Epoch 3/1000\n",
      " - 62s - loss: 0.0178 - val_loss: 0.0807\n",
      "Epoch 4/1000\n",
      " - 63s - loss: 0.0184 - val_loss: 0.0805\n",
      "Epoch 5/1000\n",
      " - 64s - loss: 0.0178 - val_loss: 0.0813\n",
      "Epoch 6/1000\n",
      " - 64s - loss: 0.0183 - val_loss: 0.0788\n",
      "Epoch 7/1000\n",
      " - 65s - loss: 0.0180 - val_loss: 0.0827\n",
      "Epoch 8/1000\n",
      " - 64s - loss: 0.0180 - val_loss: 0.0831\n",
      "Epoch 9/1000\n",
      " - 64s - loss: 0.0184 - val_loss: 0.0804\n",
      "Epoch 10/1000\n",
      " - 64s - loss: 0.0182 - val_loss: 0.0804\n",
      "Epoch 11/1000\n",
      " - 64s - loss: 0.0177 - val_loss: 0.0811\n",
      "Epoch 12/1000\n",
      " - 64s - loss: 0.0188 - val_loss: 0.0806\n",
      "Epoch 13/1000\n",
      " - 64s - loss: 0.0177 - val_loss: 0.0792\n",
      "Epoch 14/1000\n",
      " - 64s - loss: 0.0174 - val_loss: 0.0807\n",
      "Epoch 15/1000\n",
      " - 65s - loss: 0.0169 - val_loss: 0.0801\n",
      "Epoch 16/1000\n",
      " - 64s - loss: 0.0180 - val_loss: 0.0825\n",
      "Epoch 17/1000\n",
      " - 64s - loss: 0.0173 - val_loss: 0.0768\n",
      "Epoch 18/1000\n",
      " - 64s - loss: 0.0179 - val_loss: 0.0824\n",
      "Epoch 19/1000\n",
      " - 64s - loss: 0.0175 - val_loss: 0.0778\n",
      "Epoch 20/1000\n",
      " - 65s - loss: 0.0173 - val_loss: 0.0815\n",
      "Epoch 21/1000\n",
      " - 64s - loss: 0.0175 - val_loss: 0.0813\n",
      "Epoch 22/1000\n",
      " - 65s - loss: 0.0177 - val_loss: 0.0825\n",
      "Epoch 23/1000\n",
      " - 64s - loss: 0.0171 - val_loss: 0.0795\n",
      "Epoch 24/1000\n",
      " - 64s - loss: 0.0187 - val_loss: 0.0868\n",
      "Epoch 25/1000\n",
      " - 65s - loss: 0.0178 - val_loss: 0.0811\n",
      "Epoch 26/1000\n",
      " - 64s - loss: 0.0169 - val_loss: 0.0793\n",
      "Epoch 27/1000\n",
      " - 64s - loss: 0.0170 - val_loss: 0.0853\n",
      "Epoch 28/1000\n",
      " - 64s - loss: 0.0175 - val_loss: 0.0805\n",
      "Epoch 29/1000\n",
      " - 64s - loss: 0.0177 - val_loss: 0.0804\n",
      "Epoch 30/1000\n",
      " - 64s - loss: 0.0174 - val_loss: 0.0847\n",
      "Epoch 31/1000\n",
      " - 65s - loss: 0.0176 - val_loss: 0.0821\n",
      "Epoch 32/1000\n",
      " - 64s - loss: 0.0175 - val_loss: 0.0792\n",
      "Epoch 33/1000\n",
      " - 65s - loss: 0.0174 - val_loss: 0.0779\n",
      "Epoch 34/1000\n",
      " - 64s - loss: 0.0170 - val_loss: 0.0860\n",
      "Epoch 35/1000\n",
      " - 65s - loss: 0.0164 - val_loss: 0.0808\n",
      "Epoch 36/1000\n",
      " - 64s - loss: 0.0172 - val_loss: 0.0782\n",
      "Epoch 37/1000\n",
      " - 65s - loss: 0.0177 - val_loss: 0.0782\n",
      "Epoch 38/1000\n",
      " - 65s - loss: 0.0175 - val_loss: 0.0809\n",
      "Epoch 39/1000\n",
      " - 65s - loss: 0.0171 - val_loss: 0.0804\n",
      "Epoch 40/1000\n",
      " - 64s - loss: 0.0172 - val_loss: 0.0787\n",
      "Epoch 41/1000\n",
      " - 65s - loss: 0.0165 - val_loss: 0.0813\n",
      "Epoch 42/1000\n",
      " - 64s - loss: 0.0168 - val_loss: 0.0798\n",
      "Epoch 43/1000\n",
      " - 65s - loss: 0.0165 - val_loss: 0.0833\n",
      "Epoch 44/1000\n",
      " - 64s - loss: 0.0177 - val_loss: 0.0802\n",
      "Epoch 45/1000\n",
      " - 63s - loss: 0.0167 - val_loss: 0.0942\n",
      "Epoch 46/1000\n",
      " - 61s - loss: 0.0179 - val_loss: 0.0807\n",
      "Epoch 47/1000\n",
      " - 61s - loss: 0.0164 - val_loss: 0.0804\n",
      "Epoch 48/1000\n",
      " - 63s - loss: 0.0158 - val_loss: 0.0798\n",
      "Epoch 49/1000\n",
      " - 65s - loss: 0.0166 - val_loss: 0.0779\n",
      "Epoch 50/1000\n",
      " - 64s - loss: 0.0155 - val_loss: 0.0862\n",
      "Epoch 51/1000\n",
      " - 65s - loss: 0.0163 - val_loss: 0.0840\n",
      "Epoch 52/1000\n",
      " - 65s - loss: 0.0160 - val_loss: 0.0810\n",
      "Epoch 53/1000\n",
      " - 64s - loss: 0.0160 - val_loss: 0.0834\n",
      "Epoch 54/1000\n",
      " - 64s - loss: 0.0161 - val_loss: 0.0790\n",
      "Epoch 55/1000\n",
      " - 64s - loss: 0.0171 - val_loss: 0.0800\n",
      "Epoch 56/1000\n",
      " - 64s - loss: 0.0161 - val_loss: 0.0818\n",
      "Epoch 57/1000\n",
      " - 64s - loss: 0.0165 - val_loss: 0.0848\n",
      "Epoch 58/1000\n",
      " - 64s - loss: 0.0167 - val_loss: 0.0783\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-395b1224f22e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdense_unet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# directional vectors, without dropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\dti\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\dti\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\dti\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\dti\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\dti\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dense_unet.fit(train_X_n, train_Y_n, batch_size=batch_size, epochs=epochs,verbose=2,validation_split=0.2) # directional vectors, without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12152 samples, validate on 3038 samples\n",
      "Epoch 1/1000\n",
      " - 68s - loss: 0.4648 - val_loss: 0.3136\n",
      "Epoch 2/1000\n",
      " - 65s - loss: 0.1752 - val_loss: 0.2184\n",
      "Epoch 3/1000\n",
      " - 65s - loss: 0.1367 - val_loss: 0.1955\n",
      "Epoch 4/1000\n",
      " - 65s - loss: 0.1367 - val_loss: 0.1742\n",
      "Epoch 5/1000\n",
      " - 65s - loss: 0.1266 - val_loss: 0.1579\n",
      "Epoch 6/1000\n",
      " - 65s - loss: 0.1161 - val_loss: 0.1811\n",
      "Epoch 7/1000\n",
      " - 65s - loss: 0.1096 - val_loss: 0.1769\n",
      "Epoch 8/1000\n",
      " - 65s - loss: 0.1069 - val_loss: 0.1542\n",
      "Epoch 9/1000\n",
      " - 65s - loss: 0.1006 - val_loss: 0.1543\n",
      "Epoch 10/1000\n",
      " - 65s - loss: 0.0964 - val_loss: 0.1324\n",
      "Epoch 11/1000\n",
      " - 65s - loss: 0.0892 - val_loss: 0.1381\n",
      "Epoch 12/1000\n",
      " - 65s - loss: 0.0816 - val_loss: 0.1367\n",
      "Epoch 13/1000\n",
      " - 65s - loss: 0.0780 - val_loss: 0.1462\n",
      "Epoch 14/1000\n",
      " - 65s - loss: 0.0847 - val_loss: 0.1301\n",
      "Epoch 15/1000\n",
      " - 65s - loss: 0.0720 - val_loss: 0.1278\n",
      "Epoch 16/1000\n",
      " - 65s - loss: 0.0780 - val_loss: 0.1340\n",
      "Epoch 17/1000\n",
      " - 65s - loss: 0.0766 - val_loss: 0.1232\n",
      "Epoch 18/1000\n",
      " - 65s - loss: 0.0770 - val_loss: 0.1121\n",
      "Epoch 19/1000\n",
      " - 65s - loss: 0.0786 - val_loss: 0.1272\n",
      "Epoch 20/1000\n",
      " - 65s - loss: 0.0701 - val_loss: 0.1247\n",
      "Epoch 21/1000\n",
      " - 65s - loss: 0.0675 - val_loss: 0.1135\n",
      "Epoch 22/1000\n",
      " - 65s - loss: 0.0694 - val_loss: 0.1203\n",
      "Epoch 23/1000\n",
      " - 65s - loss: 0.0626 - val_loss: 0.1108\n",
      "Epoch 24/1000\n",
      " - 65s - loss: 0.0576 - val_loss: 0.1111\n",
      "Epoch 25/1000\n",
      " - 65s - loss: 0.0567 - val_loss: 0.1056\n",
      "Epoch 26/1000\n",
      " - 65s - loss: 0.0610 - val_loss: 0.1087\n",
      "Epoch 27/1000\n",
      " - 65s - loss: 0.0595 - val_loss: 0.1154\n",
      "Epoch 28/1000\n",
      " - 65s - loss: 0.0530 - val_loss: 0.1108\n",
      "Epoch 29/1000\n",
      " - 65s - loss: 0.0550 - val_loss: 0.1120\n",
      "Epoch 30/1000\n",
      " - 65s - loss: 0.0578 - val_loss: 0.1146\n",
      "Epoch 31/1000\n",
      " - 65s - loss: 0.0580 - val_loss: 0.1114\n",
      "Epoch 32/1000\n",
      " - 65s - loss: 0.0560 - val_loss: 0.1137\n",
      "Epoch 33/1000\n",
      " - 65s - loss: 0.0494 - val_loss: 0.1091\n",
      "Epoch 34/1000\n",
      " - 65s - loss: 0.0530 - val_loss: 0.1026\n",
      "Epoch 35/1000\n",
      " - 65s - loss: 0.0525 - val_loss: 0.1153\n",
      "Epoch 36/1000\n",
      " - 64s - loss: 0.0504 - val_loss: 0.0985\n",
      "Epoch 37/1000\n",
      " - 63s - loss: 0.0494 - val_loss: 0.1007\n",
      "Epoch 38/1000\n",
      " - 64s - loss: 0.0472 - val_loss: 0.0996\n",
      "Epoch 39/1000\n",
      " - 64s - loss: 0.0462 - val_loss: 0.1089\n",
      "Epoch 40/1000\n",
      " - 65s - loss: 0.0466 - val_loss: 0.1068\n",
      "Epoch 41/1000\n",
      " - 65s - loss: 0.0450 - val_loss: 0.1115\n",
      "Epoch 42/1000\n",
      " - 65s - loss: 0.0473 - val_loss: 0.1034\n",
      "Epoch 43/1000\n",
      " - 65s - loss: 0.0444 - val_loss: 0.1005\n",
      "Epoch 44/1000\n",
      " - 65s - loss: 0.0429 - val_loss: 0.0980\n",
      "Epoch 45/1000\n",
      " - 65s - loss: 0.0440 - val_loss: 0.1015\n",
      "Epoch 46/1000\n",
      " - 65s - loss: 0.0426 - val_loss: 0.1022\n",
      "Epoch 47/1000\n",
      " - 65s - loss: 0.0447 - val_loss: 0.1048\n",
      "Epoch 48/1000\n",
      " - 65s - loss: 0.0411 - val_loss: 0.1106\n",
      "Epoch 49/1000\n",
      " - 65s - loss: 0.0416 - val_loss: 0.1030\n",
      "Epoch 50/1000\n",
      " - 65s - loss: 0.0410 - val_loss: 0.1007\n",
      "Epoch 51/1000\n",
      " - 65s - loss: 0.0385 - val_loss: 0.1023\n",
      "Epoch 52/1000\n",
      " - 65s - loss: 0.0376 - val_loss: 0.0935\n",
      "Epoch 53/1000\n",
      " - 65s - loss: 0.0375 - val_loss: 0.0972\n",
      "Epoch 54/1000\n",
      " - 65s - loss: 0.0391 - val_loss: 0.1011\n",
      "Epoch 55/1000\n",
      " - 65s - loss: 0.0380 - val_loss: 0.0975\n",
      "Epoch 56/1000\n",
      " - 65s - loss: 0.0353 - val_loss: 0.1024\n",
      "Epoch 57/1000\n",
      " - 65s - loss: 0.0374 - val_loss: 0.0946\n",
      "Epoch 58/1000\n",
      " - 65s - loss: 0.0370 - val_loss: 0.0989\n",
      "Epoch 59/1000\n",
      " - 65s - loss: 0.0371 - val_loss: 0.0951\n",
      "Epoch 60/1000\n",
      " - 65s - loss: 0.0377 - val_loss: 0.0904\n",
      "Epoch 61/1000\n",
      " - 65s - loss: 0.0357 - val_loss: 0.0982\n",
      "Epoch 62/1000\n",
      " - 65s - loss: 0.0363 - val_loss: 0.0969\n",
      "Epoch 63/1000\n",
      " - 66s - loss: 0.0356 - val_loss: 0.0924\n",
      "Epoch 64/1000\n",
      " - 65s - loss: 0.0343 - val_loss: 0.0979\n",
      "Epoch 65/1000\n",
      " - 65s - loss: 0.0369 - val_loss: 0.0919\n",
      "Epoch 66/1000\n",
      " - 66s - loss: 0.0377 - val_loss: 0.0902\n",
      "Epoch 67/1000\n",
      " - 65s - loss: 0.0320 - val_loss: 0.0969\n",
      "Epoch 68/1000\n",
      " - 65s - loss: 0.0353 - val_loss: 0.0859\n",
      "Epoch 69/1000\n",
      " - 64s - loss: 0.0338 - val_loss: 0.0976\n",
      "Epoch 70/1000\n",
      " - 63s - loss: 0.0327 - val_loss: 0.1005\n",
      "Epoch 71/1000\n",
      " - 63s - loss: 0.0333 - val_loss: 0.0928\n",
      "Epoch 72/1000\n",
      " - 64s - loss: 0.0333 - val_loss: 0.0924\n",
      "Epoch 73/1000\n",
      " - 65s - loss: 0.0307 - val_loss: 0.0867\n",
      "Epoch 74/1000\n",
      " - 65s - loss: 0.0312 - val_loss: 0.0894\n",
      "Epoch 75/1000\n",
      " - 65s - loss: 0.0298 - val_loss: 0.0909\n",
      "Epoch 76/1000\n",
      " - 66s - loss: 0.0319 - val_loss: 0.0860\n",
      "Epoch 77/1000\n",
      " - 65s - loss: 0.0305 - val_loss: 0.0866\n",
      "Epoch 78/1000\n",
      " - 65s - loss: 0.0298 - val_loss: 0.0883\n",
      "Epoch 79/1000\n",
      " - 65s - loss: 0.0296 - val_loss: 0.0911\n",
      "Epoch 80/1000\n",
      " - 65s - loss: 0.0292 - val_loss: 0.0881\n",
      "Epoch 81/1000\n",
      " - 65s - loss: 0.0285 - val_loss: 0.0914\n",
      "Epoch 82/1000\n",
      " - 65s - loss: 0.0290 - val_loss: 0.0961\n",
      "Epoch 83/1000\n",
      " - 65s - loss: 0.0310 - val_loss: 0.0844\n",
      "Epoch 84/1000\n",
      " - 64s - loss: 0.0304 - val_loss: 0.0878\n",
      "Epoch 85/1000\n",
      " - 64s - loss: 0.0287 - val_loss: 0.0952\n",
      "Epoch 86/1000\n",
      " - 64s - loss: 0.0302 - val_loss: 0.0882\n",
      "Epoch 87/1000\n",
      " - 64s - loss: 0.0260 - val_loss: 0.0837\n",
      "Epoch 88/1000\n",
      " - 65s - loss: 0.0274 - val_loss: 0.0911\n",
      "Epoch 89/1000\n",
      " - 65s - loss: 0.0266 - val_loss: 0.0922\n",
      "Epoch 90/1000\n",
      " - 65s - loss: 0.0274 - val_loss: 0.0854\n",
      "Epoch 91/1000\n",
      " - 65s - loss: 0.0286 - val_loss: 0.0962\n",
      "Epoch 92/1000\n",
      " - 65s - loss: 0.0259 - val_loss: 0.0855\n",
      "Epoch 93/1000\n",
      " - 66s - loss: 0.0264 - val_loss: 0.0882\n",
      "Epoch 94/1000\n",
      " - 65s - loss: 0.0259 - val_loss: 0.0866\n",
      "Epoch 95/1000\n",
      " - 66s - loss: 0.0286 - val_loss: 0.0849\n",
      "Epoch 96/1000\n",
      " - 65s - loss: 0.0269 - val_loss: 0.0934\n",
      "Epoch 97/1000\n",
      " - 65s - loss: 0.0253 - val_loss: 0.0891\n",
      "Epoch 98/1000\n",
      " - 66s - loss: 0.0268 - val_loss: 0.0960\n",
      "Epoch 99/1000\n",
      " - 65s - loss: 0.0250 - val_loss: 0.0890\n",
      "Epoch 100/1000\n",
      " - 66s - loss: 0.0245 - val_loss: 0.0847\n",
      "Epoch 101/1000\n",
      " - 66s - loss: 0.0245 - val_loss: 0.0821\n",
      "Epoch 102/1000\n",
      " - 65s - loss: 0.0257 - val_loss: 0.0859\n",
      "Epoch 103/1000\n",
      " - 65s - loss: 0.0277 - val_loss: 0.0818\n",
      "Epoch 104/1000\n",
      " - 66s - loss: 0.0245 - val_loss: 0.0863\n",
      "Epoch 105/1000\n",
      " - 66s - loss: 0.0282 - val_loss: 0.0845\n",
      "Epoch 106/1000\n",
      " - 66s - loss: 0.0245 - val_loss: 0.0833\n",
      "Epoch 107/1000\n",
      " - 65s - loss: 0.0239 - val_loss: 0.0821\n",
      "Epoch 108/1000\n",
      " - 65s - loss: 0.0238 - val_loss: 0.0875\n",
      "Epoch 109/1000\n",
      " - 65s - loss: 0.0243 - val_loss: 0.0842\n",
      "Epoch 110/1000\n",
      " - 66s - loss: 0.0246 - val_loss: 0.0861\n",
      "Epoch 111/1000\n",
      " - 65s - loss: 0.0244 - val_loss: 0.0874\n",
      "Epoch 112/1000\n",
      " - 65s - loss: 0.0248 - val_loss: 0.0853\n",
      "Epoch 113/1000\n",
      " - 66s - loss: 0.0246 - val_loss: 0.0803\n",
      "Epoch 114/1000\n",
      " - 66s - loss: 0.0252 - val_loss: 0.0906\n",
      "Epoch 115/1000\n",
      " - 66s - loss: 0.0255 - val_loss: 0.0908\n",
      "Epoch 116/1000\n",
      " - 65s - loss: 0.0251 - val_loss: 0.0866\n",
      "Epoch 117/1000\n",
      " - 66s - loss: 0.0227 - val_loss: 0.0823\n",
      "Epoch 118/1000\n",
      " - 65s - loss: 0.0221 - val_loss: 0.0812\n",
      "Epoch 119/1000\n",
      " - 65s - loss: 0.0247 - val_loss: 0.0815\n",
      "Epoch 120/1000\n",
      " - 66s - loss: 0.0239 - val_loss: 0.0797\n",
      "Epoch 121/1000\n",
      " - 66s - loss: 0.0228 - val_loss: 0.0795\n",
      "Epoch 122/1000\n",
      " - 65s - loss: 0.0243 - val_loss: 0.0863\n",
      "Epoch 123/1000\n",
      " - 65s - loss: 0.0216 - val_loss: 0.0811\n",
      "Epoch 124/1000\n",
      " - 65s - loss: 0.0239 - val_loss: 0.0797\n",
      "Epoch 125/1000\n",
      " - 65s - loss: 0.0216 - val_loss: 0.0794\n",
      "Epoch 126/1000\n",
      " - 65s - loss: 0.0230 - val_loss: 0.0863\n",
      "Epoch 127/1000\n",
      " - 66s - loss: 0.0246 - val_loss: 0.0870\n",
      "Epoch 128/1000\n",
      " - 65s - loss: 0.0223 - val_loss: 0.0822\n",
      "Epoch 129/1000\n",
      " - 65s - loss: 0.0235 - val_loss: 0.0839\n",
      "Epoch 130/1000\n",
      " - 65s - loss: 0.0226 - val_loss: 0.0823\n",
      "Epoch 131/1000\n",
      " - 65s - loss: 0.0226 - val_loss: 0.0804\n",
      "Epoch 132/1000\n",
      " - 66s - loss: 0.0204 - val_loss: 0.0800\n",
      "Epoch 133/1000\n",
      " - 65s - loss: 0.0220 - val_loss: 0.0778\n",
      "Epoch 134/1000\n",
      " - 64s - loss: 0.0209 - val_loss: 0.0821\n",
      "Epoch 135/1000\n",
      " - 64s - loss: 0.0208 - val_loss: 0.0858\n",
      "Epoch 136/1000\n",
      " - 64s - loss: 0.0211 - val_loss: 0.0801\n",
      "Epoch 137/1000\n",
      " - 65s - loss: 0.0204 - val_loss: 0.0791\n",
      "Epoch 138/1000\n",
      " - 65s - loss: 0.0200 - val_loss: 0.0854\n",
      "Epoch 139/1000\n",
      " - 65s - loss: 0.0207 - val_loss: 0.0800\n",
      "Epoch 140/1000\n",
      " - 65s - loss: 0.0220 - val_loss: 0.0812\n",
      "Epoch 141/1000\n",
      " - 65s - loss: 0.0216 - val_loss: 0.0832\n",
      "Epoch 142/1000\n",
      " - 65s - loss: 0.0205 - val_loss: 0.0790\n",
      "Epoch 143/1000\n",
      " - 66s - loss: 0.0195 - val_loss: 0.0785\n",
      "Epoch 144/1000\n",
      " - 66s - loss: 0.0191 - val_loss: 0.0821\n",
      "Epoch 145/1000\n",
      " - 66s - loss: 0.0211 - val_loss: 0.0790\n",
      "Epoch 146/1000\n",
      " - 65s - loss: 0.0206 - val_loss: 0.0797\n",
      "Epoch 147/1000\n",
      " - 65s - loss: 0.0193 - val_loss: 0.0775\n",
      "Epoch 148/1000\n",
      " - 65s - loss: 0.0197 - val_loss: 0.0816\n",
      "Epoch 149/1000\n",
      " - 66s - loss: 0.0201 - val_loss: 0.0859\n",
      "Epoch 150/1000\n",
      " - 66s - loss: 0.0200 - val_loss: 0.0790\n",
      "Epoch 151/1000\n",
      " - 65s - loss: 0.0205 - val_loss: 0.0784\n",
      "Epoch 152/1000\n",
      " - 65s - loss: 0.0195 - val_loss: 0.0828\n",
      "Epoch 153/1000\n",
      " - 65s - loss: 0.0205 - val_loss: 0.0775\n",
      "Epoch 154/1000\n",
      " - 65s - loss: 0.0201 - val_loss: 0.0788\n",
      "Epoch 155/1000\n",
      " - 65s - loss: 0.0226 - val_loss: 0.0837\n",
      "Epoch 156/1000\n",
      " - 66s - loss: 0.0202 - val_loss: 0.0817\n",
      "Epoch 157/1000\n",
      " - 65s - loss: 0.0193 - val_loss: 0.0912\n",
      "Epoch 158/1000\n",
      " - 66s - loss: 0.0196 - val_loss: 0.0861\n",
      "Epoch 159/1000\n",
      " - 66s - loss: 0.0190 - val_loss: 0.0804\n",
      "Epoch 160/1000\n",
      " - 65s - loss: 0.0201 - val_loss: 0.0825\n",
      "Epoch 161/1000\n",
      " - 65s - loss: 0.0201 - val_loss: 0.0792\n",
      "Epoch 162/1000\n",
      " - 65s - loss: 0.0184 - val_loss: 0.0773\n",
      "Epoch 163/1000\n",
      " - 65s - loss: 0.0178 - val_loss: 0.0748\n",
      "Epoch 164/1000\n",
      " - 65s - loss: 0.0186 - val_loss: 0.0784\n",
      "Epoch 165/1000\n",
      " - 65s - loss: 0.0181 - val_loss: 0.0808\n",
      "Epoch 166/1000\n",
      " - 65s - loss: 0.0179 - val_loss: 0.0758\n",
      "Epoch 167/1000\n",
      " - 65s - loss: 0.0183 - val_loss: 0.0765\n",
      "Epoch 168/1000\n",
      " - 65s - loss: 0.0188 - val_loss: 0.0788\n",
      "Epoch 169/1000\n",
      " - 66s - loss: 0.0191 - val_loss: 0.0755\n",
      "Epoch 170/1000\n",
      " - 66s - loss: 0.0185 - val_loss: 0.0772\n",
      "Epoch 171/1000\n",
      " - 66s - loss: 0.0182 - val_loss: 0.0823\n",
      "Epoch 172/1000\n",
      " - 65s - loss: 0.0180 - val_loss: 0.0781\n",
      "Epoch 173/1000\n",
      " - 65s - loss: 0.0179 - val_loss: 0.0813\n",
      "Epoch 174/1000\n",
      " - 65s - loss: 0.0194 - val_loss: 0.0778\n",
      "Epoch 175/1000\n",
      " - 65s - loss: 0.0176 - val_loss: 0.0763\n",
      "Epoch 176/1000\n",
      " - 65s - loss: 0.0194 - val_loss: 0.0799\n",
      "Epoch 177/1000\n",
      " - 65s - loss: 0.0185 - val_loss: 0.0787\n",
      "Epoch 178/1000\n",
      " - 65s - loss: 0.0204 - val_loss: 0.0791\n",
      "Epoch 179/1000\n",
      " - 63s - loss: 0.0185 - val_loss: 0.0788\n",
      "Epoch 180/1000\n",
      " - 63s - loss: 0.0181 - val_loss: 0.0777\n",
      "Epoch 181/1000\n",
      " - 65s - loss: 0.0169 - val_loss: 0.0764\n",
      "Epoch 182/1000\n",
      " - 66s - loss: 0.0192 - val_loss: 0.0881\n",
      "Epoch 183/1000\n",
      " - 66s - loss: 0.0194 - val_loss: 0.0741\n",
      "Epoch 184/1000\n",
      " - 65s - loss: 0.0178 - val_loss: 0.0758\n",
      "Epoch 185/1000\n",
      " - 66s - loss: 0.0191 - val_loss: 0.0758\n",
      "Epoch 186/1000\n",
      " - 66s - loss: 0.0171 - val_loss: 0.0757\n",
      "Epoch 187/1000\n",
      " - 66s - loss: 0.0193 - val_loss: 0.0780\n",
      "Epoch 188/1000\n",
      " - 65s - loss: 0.0175 - val_loss: 0.0767\n",
      "Epoch 189/1000\n",
      " - 65s - loss: 0.0171 - val_loss: 0.0746\n",
      "Epoch 190/1000\n",
      " - 66s - loss: 0.0166 - val_loss: 0.0756\n",
      "Epoch 191/1000\n",
      " - 65s - loss: 0.0188 - val_loss: 0.0766\n",
      "Epoch 192/1000\n",
      " - 65s - loss: 0.0174 - val_loss: 0.0815\n",
      "Epoch 193/1000\n",
      " - 65s - loss: 0.0179 - val_loss: 0.0739\n",
      "Epoch 194/1000\n",
      " - 66s - loss: 0.0162 - val_loss: 0.0796\n",
      "Epoch 195/1000\n",
      " - 65s - loss: 0.0165 - val_loss: 0.0771\n",
      "Epoch 196/1000\n",
      " - 66s - loss: 0.0172 - val_loss: 0.0759\n",
      "Epoch 197/1000\n",
      " - 65s - loss: 0.0171 - val_loss: 0.0823\n",
      "Epoch 198/1000\n",
      " - 66s - loss: 0.0176 - val_loss: 0.0765\n",
      "Epoch 199/1000\n",
      " - 66s - loss: 0.0177 - val_loss: 0.0739\n",
      "Epoch 200/1000\n",
      " - 65s - loss: 0.0171 - val_loss: 0.0720\n",
      "Epoch 201/1000\n",
      " - 66s - loss: 0.0176 - val_loss: 0.0710\n",
      "Epoch 202/1000\n",
      " - 65s - loss: 0.0162 - val_loss: 0.0754\n",
      "Epoch 203/1000\n",
      " - 65s - loss: 0.0153 - val_loss: 0.0748\n",
      "Epoch 204/1000\n",
      " - 65s - loss: 0.0154 - val_loss: 0.0804\n",
      "Epoch 205/1000\n",
      " - 65s - loss: 0.0159 - val_loss: 0.0719\n",
      "Epoch 206/1000\n",
      " - 64s - loss: 0.0163 - val_loss: 0.0762\n",
      "Epoch 207/1000\n",
      " - 65s - loss: 0.0162 - val_loss: 0.0767\n",
      "Epoch 208/1000\n",
      " - 65s - loss: 0.0167 - val_loss: 0.0864\n",
      "Epoch 209/1000\n",
      " - 64s - loss: 0.0162 - val_loss: 0.0750\n",
      "Epoch 210/1000\n",
      " - 64s - loss: 0.0156 - val_loss: 0.0726\n",
      "Epoch 211/1000\n",
      " - 64s - loss: 0.0149 - val_loss: 0.0729\n",
      "Epoch 212/1000\n",
      " - 65s - loss: 0.0184 - val_loss: 0.0734\n",
      "Epoch 213/1000\n",
      " - 65s - loss: 0.0167 - val_loss: 0.0800\n",
      "Epoch 214/1000\n",
      " - 65s - loss: 0.0160 - val_loss: 0.0721\n",
      "Epoch 215/1000\n",
      " - 65s - loss: 0.0171 - val_loss: 0.0742\n",
      "Epoch 216/1000\n",
      " - 64s - loss: 0.0165 - val_loss: 0.0823\n",
      "Epoch 217/1000\n",
      " - 64s - loss: 0.0166 - val_loss: 0.0724\n",
      "Epoch 218/1000\n",
      " - 65s - loss: 0.0164 - val_loss: 0.0724\n",
      "Epoch 219/1000\n",
      " - 64s - loss: 0.0168 - val_loss: 0.0770\n",
      "Epoch 220/1000\n",
      " - 65s - loss: 0.0163 - val_loss: 0.0729\n",
      "Epoch 221/1000\n",
      " - 64s - loss: 0.0154 - val_loss: 0.0777\n",
      "Epoch 222/1000\n",
      " - 65s - loss: 0.0157 - val_loss: 0.0742\n",
      "Epoch 223/1000\n",
      " - 66s - loss: 0.0152 - val_loss: 0.0759\n",
      "Epoch 224/1000\n",
      " - 65s - loss: 0.0157 - val_loss: 0.0785\n",
      "Epoch 225/1000\n",
      " - 64s - loss: 0.0153 - val_loss: 0.0742\n",
      "Epoch 226/1000\n",
      " - 64s - loss: 0.0149 - val_loss: 0.0709\n",
      "Epoch 227/1000\n",
      " - 64s - loss: 0.0160 - val_loss: 0.0720\n",
      "Epoch 228/1000\n",
      " - 65s - loss: 0.0156 - val_loss: 0.0707\n",
      "Epoch 229/1000\n",
      " - 66s - loss: 0.0160 - val_loss: 0.0738\n",
      "Epoch 230/1000\n",
      " - 66s - loss: 0.0148 - val_loss: 0.0707\n",
      "Epoch 231/1000\n",
      " - 65s - loss: 0.0162 - val_loss: 0.0724\n",
      "Epoch 232/1000\n",
      " - 66s - loss: 0.0154 - val_loss: 0.0699\n",
      "Epoch 233/1000\n",
      " - 66s - loss: 0.0157 - val_loss: 0.0787\n",
      "Epoch 234/1000\n",
      " - 65s - loss: 0.0154 - val_loss: 0.0745\n",
      "Epoch 235/1000\n",
      " - 66s - loss: 0.0166 - val_loss: 0.0741\n",
      "Epoch 236/1000\n",
      " - 65s - loss: 0.0161 - val_loss: 0.0708\n",
      "Epoch 237/1000\n",
      " - 65s - loss: 0.0159 - val_loss: 0.0728\n",
      "Epoch 238/1000\n",
      " - 66s - loss: 0.0141 - val_loss: 0.0819\n",
      "Epoch 239/1000\n",
      " - 66s - loss: 0.0149 - val_loss: 0.0715\n",
      "Epoch 240/1000\n",
      " - 65s - loss: 0.0154 - val_loss: 0.0716\n",
      "Epoch 241/1000\n",
      " - 65s - loss: 0.0145 - val_loss: 0.0717\n",
      "Epoch 242/1000\n",
      " - 66s - loss: 0.0150 - val_loss: 0.0751\n",
      "Epoch 243/1000\n",
      " - 65s - loss: 0.0147 - val_loss: 0.0751\n",
      "Epoch 244/1000\n",
      " - 65s - loss: 0.0144 - val_loss: 0.0726\n",
      "Epoch 245/1000\n",
      " - 65s - loss: 0.0149 - val_loss: 0.0696\n",
      "Epoch 246/1000\n",
      " - 66s - loss: 0.0143 - val_loss: 0.0737\n",
      "Epoch 247/1000\n",
      " - 66s - loss: 0.0150 - val_loss: 0.0746\n",
      "Epoch 248/1000\n",
      " - 66s - loss: 0.0143 - val_loss: 0.0743\n",
      "Epoch 249/1000\n",
      " - 65s - loss: 0.0141 - val_loss: 0.0730\n",
      "Epoch 250/1000\n",
      " - 66s - loss: 0.0145 - val_loss: 0.0725\n",
      "Epoch 251/1000\n",
      " - 65s - loss: 0.0144 - val_loss: 0.0792\n",
      "Epoch 252/1000\n",
      " - 66s - loss: 0.0147 - val_loss: 0.0729\n",
      "Epoch 253/1000\n",
      " - 65s - loss: 0.0147 - val_loss: 0.0712\n",
      "Epoch 254/1000\n",
      " - 65s - loss: 0.0143 - val_loss: 0.0754\n",
      "Epoch 255/1000\n",
      " - 66s - loss: 0.0138 - val_loss: 0.0710\n",
      "Epoch 256/1000\n",
      " - 65s - loss: 0.0142 - val_loss: 0.0770\n",
      "Epoch 257/1000\n",
      " - 65s - loss: 0.0138 - val_loss: 0.0696\n",
      "Epoch 258/1000\n",
      " - 65s - loss: 0.0136 - val_loss: 0.0771\n",
      "Epoch 259/1000\n",
      " - 66s - loss: 0.0142 - val_loss: 0.0730\n",
      "Epoch 260/1000\n",
      " - 65s - loss: 0.0138 - val_loss: 0.0689\n",
      "Epoch 261/1000\n",
      " - 65s - loss: 0.0155 - val_loss: 0.0781\n",
      "Epoch 262/1000\n",
      " - 65s - loss: 0.0136 - val_loss: 0.0817\n",
      "Epoch 263/1000\n",
      " - 66s - loss: 0.0132 - val_loss: 0.0696\n",
      "Epoch 264/1000\n",
      " - 66s - loss: 0.0136 - val_loss: 0.0680\n",
      "Epoch 265/1000\n",
      " - 66s - loss: 0.0138 - val_loss: 0.0682\n",
      "Epoch 266/1000\n",
      " - 66s - loss: 0.0140 - val_loss: 0.0725\n",
      "Epoch 267/1000\n",
      " - 65s - loss: 0.0141 - val_loss: 0.0717\n",
      "Epoch 268/1000\n",
      " - 66s - loss: 0.0138 - val_loss: 0.0719\n",
      "Epoch 269/1000\n",
      " - 65s - loss: 0.0135 - val_loss: 0.0707\n",
      "Epoch 270/1000\n",
      " - 66s - loss: 0.0134 - val_loss: 0.0728\n",
      "Epoch 271/1000\n",
      " - 64s - loss: 0.0146 - val_loss: 0.0717\n",
      "Epoch 272/1000\n",
      " - 64s - loss: 0.0146 - val_loss: 0.0692\n",
      "Epoch 273/1000\n",
      " - 63s - loss: 0.0143 - val_loss: 0.0673\n",
      "Epoch 274/1000\n",
      " - 63s - loss: 0.0140 - val_loss: 0.0750\n",
      "Epoch 275/1000\n",
      " - 65s - loss: 0.0132 - val_loss: 0.0698\n",
      "Epoch 276/1000\n",
      " - 66s - loss: 0.0132 - val_loss: 0.0722\n",
      "Epoch 277/1000\n",
      " - 65s - loss: 0.0140 - val_loss: 0.0737\n",
      "Epoch 278/1000\n",
      " - 66s - loss: 0.0131 - val_loss: 0.0725\n",
      "Epoch 279/1000\n",
      " - 66s - loss: 0.0135 - val_loss: 0.0673\n",
      "Epoch 280/1000\n",
      " - 66s - loss: 0.0137 - val_loss: 0.0732\n",
      "Epoch 281/1000\n",
      " - 65s - loss: 0.0139 - val_loss: 0.0699\n",
      "Epoch 282/1000\n",
      " - 66s - loss: 0.0131 - val_loss: 0.0735\n",
      "Epoch 283/1000\n",
      " - 66s - loss: 0.0135 - val_loss: 0.0719\n",
      "Epoch 284/1000\n",
      " - 66s - loss: 0.0138 - val_loss: 0.0757\n",
      "Epoch 285/1000\n",
      " - 65s - loss: 0.0139 - val_loss: 0.0700\n",
      "Epoch 286/1000\n",
      " - 66s - loss: 0.0140 - val_loss: 0.0727\n",
      "Epoch 287/1000\n",
      " - 66s - loss: 0.0138 - val_loss: 0.0658\n",
      "Epoch 288/1000\n",
      " - 66s - loss: 0.0142 - val_loss: 0.0686\n",
      "Epoch 289/1000\n",
      " - 65s - loss: 0.0129 - val_loss: 0.0769\n",
      "Epoch 290/1000\n",
      " - 66s - loss: 0.0126 - val_loss: 0.0671\n",
      "Epoch 291/1000\n",
      " - 66s - loss: 0.0131 - val_loss: 0.0721\n",
      "Epoch 292/1000\n",
      " - 66s - loss: 0.0136 - val_loss: 0.0693\n",
      "Epoch 293/1000\n",
      " - 66s - loss: 0.0129 - val_loss: 0.0700\n",
      "Epoch 294/1000\n",
      " - 66s - loss: 0.0127 - val_loss: 0.0743\n",
      "Epoch 295/1000\n",
      " - 65s - loss: 0.0131 - val_loss: 0.0715\n",
      "Epoch 296/1000\n",
      " - 65s - loss: 0.0125 - val_loss: 0.0675\n",
      "Epoch 297/1000\n",
      " - 65s - loss: 0.0127 - val_loss: 0.0679\n",
      "Epoch 298/1000\n",
      " - 65s - loss: 0.0132 - val_loss: 0.0722\n",
      "Epoch 299/1000\n",
      " - 65s - loss: 0.0136 - val_loss: 0.0713\n",
      "Epoch 300/1000\n",
      " - 66s - loss: 0.0137 - val_loss: 0.0705\n",
      "Epoch 301/1000\n",
      " - 65s - loss: 0.0126 - val_loss: 0.0703\n",
      "Epoch 302/1000\n",
      " - 65s - loss: 0.0127 - val_loss: 0.0679\n",
      "Epoch 303/1000\n",
      " - 66s - loss: 0.0133 - val_loss: 0.0665\n",
      "Epoch 304/1000\n",
      " - 66s - loss: 0.0121 - val_loss: 0.0717\n",
      "Epoch 305/1000\n",
      " - 66s - loss: 0.0126 - val_loss: 0.0674\n",
      "Epoch 306/1000\n",
      " - 65s - loss: 0.0134 - val_loss: 0.0686\n",
      "Epoch 307/1000\n",
      " - 65s - loss: 0.0133 - val_loss: 0.0675\n",
      "Epoch 308/1000\n",
      " - 65s - loss: 0.0130 - val_loss: 0.0707\n",
      "Epoch 309/1000\n",
      " - 65s - loss: 0.0138 - val_loss: 0.0699\n",
      "Epoch 310/1000\n",
      " - 65s - loss: 0.0127 - val_loss: 0.0676\n",
      "Epoch 311/1000\n",
      " - 65s - loss: 0.0128 - val_loss: 0.0726\n",
      "Epoch 312/1000\n",
      " - 66s - loss: 0.0133 - val_loss: 0.0666\n",
      "Epoch 313/1000\n",
      " - 66s - loss: 0.0127 - val_loss: 0.0806\n",
      "Epoch 314/1000\n",
      " - 65s - loss: 0.0138 - val_loss: 0.0728\n",
      "Epoch 315/1000\n",
      " - 65s - loss: 0.0120 - val_loss: 0.0715\n",
      "Epoch 316/1000\n",
      " - 66s - loss: 0.0129 - val_loss: 0.0707\n",
      "Epoch 317/1000\n",
      " - 65s - loss: 0.0120 - val_loss: 0.0723\n",
      "Epoch 318/1000\n",
      " - 65s - loss: 0.0124 - val_loss: 0.0674\n",
      "Epoch 319/1000\n",
      " - 65s - loss: 0.0123 - val_loss: 0.0746\n",
      "Epoch 320/1000\n",
      " - 66s - loss: 0.0125 - val_loss: 0.0667\n",
      "Epoch 321/1000\n",
      " - 65s - loss: 0.0118 - val_loss: 0.0672\n",
      "Epoch 322/1000\n",
      " - 64s - loss: 0.0132 - val_loss: 0.0673\n",
      "Epoch 323/1000\n",
      " - 64s - loss: 0.0141 - val_loss: 0.0666\n",
      "Epoch 324/1000\n",
      " - 64s - loss: 0.0121 - val_loss: 0.0689\n",
      "Epoch 325/1000\n",
      " - 65s - loss: 0.0121 - val_loss: 0.0675\n",
      "Epoch 326/1000\n",
      " - 65s - loss: 0.0121 - val_loss: 0.0718\n",
      "Epoch 327/1000\n",
      " - 66s - loss: 0.0129 - val_loss: 0.0712\n",
      "Epoch 328/1000\n",
      " - 65s - loss: 0.0120 - val_loss: 0.0709\n",
      "Epoch 329/1000\n",
      " - 65s - loss: 0.0127 - val_loss: 0.0703\n",
      "Epoch 330/1000\n",
      " - 66s - loss: 0.0120 - val_loss: 0.0719\n",
      "Epoch 331/1000\n",
      " - 65s - loss: 0.0116 - val_loss: 0.0652\n",
      "Epoch 332/1000\n",
      " - 65s - loss: 0.0114 - val_loss: 0.0663\n",
      "Epoch 333/1000\n",
      " - 66s - loss: 0.0119 - val_loss: 0.0710\n",
      "Epoch 334/1000\n",
      " - 65s - loss: 0.0120 - val_loss: 0.0699\n",
      "Epoch 335/1000\n",
      " - 66s - loss: 0.0119 - val_loss: 0.0675\n",
      "Epoch 336/1000\n",
      " - 66s - loss: 0.0116 - val_loss: 0.0730\n",
      "Epoch 337/1000\n",
      " - 66s - loss: 0.0110 - val_loss: 0.0702\n",
      "Epoch 338/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0687\n",
      "Epoch 339/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0657\n",
      "Epoch 340/1000\n",
      " - 65s - loss: 0.0123 - val_loss: 0.0698\n",
      "Epoch 341/1000\n",
      " - 65s - loss: 0.0121 - val_loss: 0.0708\n",
      "Epoch 342/1000\n",
      " - 66s - loss: 0.0118 - val_loss: 0.0681\n",
      "Epoch 343/1000\n",
      " - 66s - loss: 0.0117 - val_loss: 0.0696\n",
      "Epoch 344/1000\n",
      " - 65s - loss: 0.0116 - val_loss: 0.0732\n",
      "Epoch 345/1000\n",
      " - 66s - loss: 0.0109 - val_loss: 0.0637\n",
      "Epoch 346/1000\n",
      " - 65s - loss: 0.0117 - val_loss: 0.0686\n",
      "Epoch 347/1000\n",
      " - 66s - loss: 0.0119 - val_loss: 0.0677\n",
      "Epoch 348/1000\n",
      " - 66s - loss: 0.0110 - val_loss: 0.0674\n",
      "Epoch 349/1000\n",
      " - 66s - loss: 0.0123 - val_loss: 0.0683\n",
      "Epoch 350/1000\n",
      " - 66s - loss: 0.0114 - val_loss: 0.0691\n",
      "Epoch 351/1000\n",
      " - 66s - loss: 0.0118 - val_loss: 0.0719\n",
      "Epoch 352/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0653\n",
      "Epoch 353/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0672\n",
      "Epoch 354/1000\n",
      " - 66s - loss: 0.0105 - val_loss: 0.0684\n",
      "Epoch 355/1000\n",
      " - 65s - loss: 0.0121 - val_loss: 0.0678\n",
      "Epoch 356/1000\n",
      " - 66s - loss: 0.0124 - val_loss: 0.0647\n",
      "Epoch 357/1000\n",
      " - 66s - loss: 0.0109 - val_loss: 0.0669\n",
      "Epoch 358/1000\n",
      " - 66s - loss: 0.0127 - val_loss: 0.0730\n",
      "Epoch 359/1000\n",
      " - 65s - loss: 0.0108 - val_loss: 0.0693\n",
      "Epoch 360/1000\n",
      " - 66s - loss: 0.0119 - val_loss: 0.0695\n",
      "Epoch 361/1000\n",
      " - 65s - loss: 0.0121 - val_loss: 0.0733\n",
      "Epoch 362/1000\n",
      " - 66s - loss: 0.0137 - val_loss: 0.0684\n",
      "Epoch 363/1000\n",
      " - 66s - loss: 0.0136 - val_loss: 0.0646\n",
      "Epoch 364/1000\n",
      " - 66s - loss: 0.0113 - val_loss: 0.0689\n",
      "Epoch 365/1000\n",
      " - 65s - loss: 0.0116 - val_loss: 0.0760\n",
      "Epoch 366/1000\n",
      " - 64s - loss: 0.0117 - val_loss: 0.0694\n",
      "Epoch 367/1000\n",
      " - 64s - loss: 0.0106 - val_loss: 0.0665\n",
      "Epoch 368/1000\n",
      " - 64s - loss: 0.0104 - val_loss: 0.0681\n",
      "Epoch 369/1000\n",
      " - 64s - loss: 0.0104 - val_loss: 0.0691\n",
      "Epoch 370/1000\n",
      " - 65s - loss: 0.0113 - val_loss: 0.0678\n",
      "Epoch 371/1000\n",
      " - 66s - loss: 0.0110 - val_loss: 0.0687\n",
      "Epoch 372/1000\n",
      " - 66s - loss: 0.0104 - val_loss: 0.0666\n",
      "Epoch 373/1000\n",
      " - 66s - loss: 0.0101 - val_loss: 0.0648\n",
      "Epoch 374/1000\n",
      " - 65s - loss: 0.0104 - val_loss: 0.0725\n",
      "Epoch 375/1000\n",
      " - 66s - loss: 0.0111 - val_loss: 0.0730\n",
      "Epoch 376/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0688\n",
      "Epoch 377/1000\n",
      " - 65s - loss: 0.0111 - val_loss: 0.0676\n",
      "Epoch 378/1000\n",
      " - 65s - loss: 0.0115 - val_loss: 0.0663\n",
      "Epoch 379/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0690\n",
      "Epoch 380/1000\n",
      " - 63s - loss: 0.0110 - val_loss: 0.0664\n",
      "Epoch 381/1000\n",
      " - 63s - loss: 0.0109 - val_loss: 0.0701\n",
      "Epoch 382/1000\n",
      " - 64s - loss: 0.0107 - val_loss: 0.0682\n",
      "Epoch 383/1000\n",
      " - 65s - loss: 0.0116 - val_loss: 0.0635\n",
      "Epoch 384/1000\n",
      " - 66s - loss: 0.0113 - val_loss: 0.0720\n",
      "Epoch 385/1000\n",
      " - 66s - loss: 0.0118 - val_loss: 0.0700\n",
      "Epoch 386/1000\n",
      " - 66s - loss: 0.0113 - val_loss: 0.0670\n",
      "Epoch 387/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0666\n",
      "Epoch 388/1000\n",
      " - 65s - loss: 0.0100 - val_loss: 0.0665\n",
      "Epoch 389/1000\n",
      " - 65s - loss: 0.0107 - val_loss: 0.0651\n",
      "Epoch 390/1000\n",
      " - 65s - loss: 0.0100 - val_loss: 0.0650\n",
      "Epoch 391/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0672\n",
      "Epoch 392/1000\n",
      " - 66s - loss: 0.0112 - val_loss: 0.0645\n",
      "Epoch 393/1000\n",
      " - 65s - loss: 0.0125 - val_loss: 0.0710\n",
      "Epoch 394/1000\n",
      " - 65s - loss: 0.0108 - val_loss: 0.0677\n",
      "Epoch 395/1000\n",
      " - 66s - loss: 0.0109 - val_loss: 0.0677\n",
      "Epoch 396/1000\n",
      " - 65s - loss: 0.0106 - val_loss: 0.0669\n",
      "Epoch 397/1000\n",
      " - 65s - loss: 0.0104 - val_loss: 0.0660\n",
      "Epoch 398/1000\n",
      " - 65s - loss: 0.0103 - val_loss: 0.0669\n",
      "Epoch 399/1000\n",
      " - 65s - loss: 0.0108 - val_loss: 0.0678\n",
      "Epoch 400/1000\n",
      " - 66s - loss: 0.0100 - val_loss: 0.0657\n",
      "Epoch 401/1000\n",
      " - 65s - loss: 0.0101 - val_loss: 0.0692\n",
      "Epoch 402/1000\n",
      " - 66s - loss: 0.0112 - val_loss: 0.0702\n",
      "Epoch 403/1000\n",
      " - 65s - loss: 0.0102 - val_loss: 0.0643\n",
      "Epoch 404/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0663\n",
      "Epoch 405/1000\n",
      " - 66s - loss: 0.0101 - val_loss: 0.0751\n",
      "Epoch 406/1000\n",
      " - 65s - loss: 0.0109 - val_loss: 0.0708\n",
      "Epoch 407/1000\n",
      " - 66s - loss: 0.0103 - val_loss: 0.0707\n",
      "Epoch 408/1000\n",
      " - 66s - loss: 0.0101 - val_loss: 0.0655\n",
      "Epoch 409/1000\n",
      " - 66s - loss: 0.0108 - val_loss: 0.0667\n",
      "Epoch 410/1000\n",
      " - 66s - loss: 0.0113 - val_loss: 0.0660\n",
      "Epoch 411/1000\n",
      " - 65s - loss: 0.0101 - val_loss: 0.0667\n",
      "Epoch 412/1000\n",
      " - 66s - loss: 0.0101 - val_loss: 0.0674\n",
      "Epoch 413/1000\n",
      " - 65s - loss: 0.0098 - val_loss: 0.0659\n",
      "Epoch 414/1000\n",
      " - 65s - loss: 0.0099 - val_loss: 0.0640\n",
      "Epoch 415/1000\n",
      " - 66s - loss: 0.0101 - val_loss: 0.0678\n",
      "Epoch 416/1000\n",
      " - 66s - loss: 0.0097 - val_loss: 0.0640\n",
      "Epoch 417/1000\n",
      " - 67s - loss: 0.0099 - val_loss: 0.0653\n",
      "Epoch 418/1000\n",
      " - 66s - loss: 0.0099 - val_loss: 0.0691\n",
      "Epoch 419/1000\n",
      " - 65s - loss: 0.0097 - val_loss: 0.0668\n",
      "Epoch 420/1000\n",
      " - 66s - loss: 0.0096 - val_loss: 0.0655\n",
      "Epoch 421/1000\n",
      " - 65s - loss: 0.0097 - val_loss: 0.0711\n",
      "Epoch 422/1000\n",
      " - 65s - loss: 0.0099 - val_loss: 0.0714\n",
      "Epoch 423/1000\n",
      " - 66s - loss: 0.0102 - val_loss: 0.0670\n",
      "Epoch 424/1000\n",
      " - 66s - loss: 0.0103 - val_loss: 0.0667\n",
      "Epoch 425/1000\n",
      " - 66s - loss: 0.0103 - val_loss: 0.0646\n",
      "Epoch 426/1000\n",
      " - 66s - loss: 0.0099 - val_loss: 0.0692\n",
      "Epoch 427/1000\n",
      " - 63s - loss: 0.0104 - val_loss: 0.0680\n",
      "Epoch 428/1000\n",
      " - 63s - loss: 0.0096 - val_loss: 0.0640\n",
      "Epoch 429/1000\n",
      " - 63s - loss: 0.0092 - val_loss: 0.0680\n",
      "Epoch 430/1000\n",
      " - 65s - loss: 0.0101 - val_loss: 0.0681\n",
      "Epoch 431/1000\n",
      " - 65s - loss: 0.0098 - val_loss: 0.0660\n",
      "Epoch 432/1000\n",
      " - 66s - loss: 0.0106 - val_loss: 0.0668\n",
      "Epoch 433/1000\n",
      " - 66s - loss: 0.0098 - val_loss: 0.0670\n",
      "Epoch 434/1000\n",
      " - 65s - loss: 0.0104 - val_loss: 0.0660\n",
      "Epoch 435/1000\n",
      " - 66s - loss: 0.0100 - val_loss: 0.0639\n",
      "Epoch 436/1000\n",
      " - 65s - loss: 0.0108 - val_loss: 0.0645\n",
      "Epoch 437/1000\n",
      " - 66s - loss: 0.0098 - val_loss: 0.0698\n",
      "Epoch 438/1000\n",
      " - 65s - loss: 0.0102 - val_loss: 0.0705\n",
      "Epoch 439/1000\n",
      " - 65s - loss: 0.0095 - val_loss: 0.0694\n",
      "Epoch 440/1000\n",
      " - 66s - loss: 0.0099 - val_loss: 0.0695\n",
      "Epoch 441/1000\n",
      " - 65s - loss: 0.0091 - val_loss: 0.0646\n",
      "Epoch 442/1000\n",
      " - 66s - loss: 0.0099 - val_loss: 0.0630\n",
      "Epoch 443/1000\n",
      " - 65s - loss: 0.0103 - val_loss: 0.0630\n",
      "Epoch 444/1000\n",
      " - 65s - loss: 0.0105 - val_loss: 0.0677\n",
      "Epoch 445/1000\n",
      " - 66s - loss: 0.0106 - val_loss: 0.0668\n",
      "Epoch 446/1000\n",
      " - 65s - loss: 0.0102 - val_loss: 0.0635\n",
      "Epoch 447/1000\n",
      " - 65s - loss: 0.0090 - val_loss: 0.0668\n",
      "Epoch 448/1000\n",
      " - 65s - loss: 0.0104 - val_loss: 0.0639\n",
      "Epoch 449/1000\n",
      " - 65s - loss: 0.0092 - val_loss: 0.0642\n",
      "Epoch 450/1000\n",
      " - 65s - loss: 0.0094 - val_loss: 0.0650\n",
      "Epoch 451/1000\n",
      " - 65s - loss: 0.0094 - val_loss: 0.0679\n",
      "Epoch 452/1000\n",
      " - 66s - loss: 0.0092 - val_loss: 0.0664\n",
      "Epoch 453/1000\n",
      " - 65s - loss: 0.0093 - val_loss: 0.0675\n",
      "Epoch 454/1000\n",
      " - 66s - loss: 0.0091 - val_loss: 0.0663\n",
      "Epoch 455/1000\n",
      " - 65s - loss: 0.0107 - val_loss: 0.0698\n",
      "Epoch 456/1000\n",
      " - 66s - loss: 0.0098 - val_loss: 0.0645\n",
      "Epoch 457/1000\n",
      " - 65s - loss: 0.0091 - val_loss: 0.0683\n",
      "Epoch 458/1000\n",
      " - 65s - loss: 0.0101 - val_loss: 0.0644\n",
      "Epoch 459/1000\n",
      " - 65s - loss: 0.0097 - val_loss: 0.0684\n",
      "Epoch 460/1000\n",
      " - 66s - loss: 0.0093 - val_loss: 0.0694\n",
      "Epoch 461/1000\n",
      " - 65s - loss: 0.0097 - val_loss: 0.0684\n",
      "Epoch 462/1000\n",
      " - 66s - loss: 0.0097 - val_loss: 0.0640\n",
      "Epoch 463/1000\n",
      " - 65s - loss: 0.0104 - val_loss: 0.0662\n",
      "Epoch 464/1000\n",
      " - 66s - loss: 0.0093 - val_loss: 0.0624\n",
      "Epoch 465/1000\n",
      " - 65s - loss: 0.0095 - val_loss: 0.0644\n",
      "Epoch 466/1000\n",
      " - 65s - loss: 0.0094 - val_loss: 0.0636\n",
      "Epoch 467/1000\n",
      " - 65s - loss: 0.0092 - val_loss: 0.0679\n",
      "Epoch 468/1000\n",
      " - 65s - loss: 0.0096 - val_loss: 0.0660\n",
      "Epoch 469/1000\n",
      " - 65s - loss: 0.0094 - val_loss: 0.0645\n",
      "Epoch 470/1000\n",
      " - 65s - loss: 0.0085 - val_loss: 0.0677\n",
      "Epoch 471/1000\n",
      " - 65s - loss: 0.0089 - val_loss: 0.0660\n",
      "Epoch 472/1000\n",
      " - 66s - loss: 0.0096 - val_loss: 0.0662\n",
      "Epoch 473/1000\n",
      " - 65s - loss: 0.0091 - val_loss: 0.0671\n",
      "Epoch 474/1000\n",
      " - 65s - loss: 0.0107 - val_loss: 0.0681\n",
      "Epoch 475/1000\n",
      " - 65s - loss: 0.0100 - val_loss: 0.0633\n",
      "Epoch 476/1000\n",
      " - 65s - loss: 0.0095 - val_loss: 0.0645\n",
      "Epoch 477/1000\n",
      " - 65s - loss: 0.0088 - val_loss: 0.0669\n",
      "Epoch 478/1000\n",
      " - 65s - loss: 0.0088 - val_loss: 0.0646\n",
      "Epoch 479/1000\n",
      " - 64s - loss: 0.0090 - val_loss: 0.0650\n",
      "Epoch 480/1000\n",
      " - 64s - loss: 0.0093 - val_loss: 0.0649\n",
      "Epoch 481/1000\n",
      " - 65s - loss: 0.0087 - val_loss: 0.0648\n",
      "Epoch 482/1000\n",
      " - 65s - loss: 0.0086 - val_loss: 0.0646\n",
      "Epoch 483/1000\n",
      " - 66s - loss: 0.0090 - val_loss: 0.0673\n",
      "Epoch 484/1000\n",
      " - 65s - loss: 0.0098 - val_loss: 0.0658\n",
      "Epoch 485/1000\n",
      " - 66s - loss: 0.0090 - val_loss: 0.0662\n",
      "Epoch 486/1000\n",
      " - 66s - loss: 0.0088 - val_loss: 0.0687\n",
      "Epoch 487/1000\n",
      " - 66s - loss: 0.0091 - val_loss: 0.0651\n",
      "Epoch 488/1000\n",
      " - 66s - loss: 0.0088 - val_loss: 0.0676\n",
      "Epoch 489/1000\n",
      " - 65s - loss: 0.0091 - val_loss: 0.0652\n",
      "Epoch 490/1000\n",
      " - 65s - loss: 0.0094 - val_loss: 0.0635\n",
      "Epoch 491/1000\n",
      " - 65s - loss: 0.0089 - val_loss: 0.0640\n",
      "Epoch 492/1000\n",
      " - 65s - loss: 0.0092 - val_loss: 0.0685\n",
      "Epoch 493/1000\n",
      " - 65s - loss: 0.0085 - val_loss: 0.0657\n",
      "Epoch 494/1000\n",
      " - 65s - loss: 0.0088 - val_loss: 0.0652\n",
      "Epoch 495/1000\n",
      " - 65s - loss: 0.0096 - val_loss: 0.0649\n",
      "Epoch 496/1000\n",
      " - 65s - loss: 0.0088 - val_loss: 0.0666\n",
      "Epoch 497/1000\n",
      " - 65s - loss: 0.0090 - val_loss: 0.0653\n",
      "Epoch 498/1000\n",
      " - 66s - loss: 0.0089 - val_loss: 0.0654\n",
      "Epoch 499/1000\n",
      " - 65s - loss: 0.0091 - val_loss: 0.0700\n",
      "Epoch 500/1000\n",
      " - 66s - loss: 0.0085 - val_loss: 0.0636\n",
      "Epoch 501/1000\n",
      " - 65s - loss: 0.0092 - val_loss: 0.0670\n",
      "Epoch 502/1000\n",
      " - 66s - loss: 0.0087 - val_loss: 0.0621\n",
      "Epoch 503/1000\n",
      " - 65s - loss: 0.0087 - val_loss: 0.0657\n",
      "Epoch 504/1000\n",
      " - 66s - loss: 0.0092 - val_loss: 0.0640\n",
      "Epoch 505/1000\n",
      " - 65s - loss: 0.0090 - val_loss: 0.0624\n",
      "Epoch 506/1000\n",
      " - 65s - loss: 0.0085 - val_loss: 0.0636\n",
      "Epoch 507/1000\n",
      " - 66s - loss: 0.0092 - val_loss: 0.0637\n",
      "Epoch 508/1000\n",
      " - 65s - loss: 0.0093 - val_loss: 0.0672\n",
      "Epoch 509/1000\n",
      " - 66s - loss: 0.0089 - val_loss: 0.0648\n",
      "Epoch 510/1000\n",
      " - 65s - loss: 0.0087 - val_loss: 0.0655\n",
      "Epoch 511/1000\n",
      " - 66s - loss: 0.0087 - val_loss: 0.0631\n",
      "Epoch 512/1000\n",
      " - 65s - loss: 0.0084 - val_loss: 0.0704\n",
      "Epoch 513/1000\n",
      " - 66s - loss: 0.0088 - val_loss: 0.0640\n",
      "Epoch 514/1000\n",
      " - 66s - loss: 0.0085 - val_loss: 0.0630\n",
      "Epoch 515/1000\n",
      " - 66s - loss: 0.0083 - val_loss: 0.0650\n",
      "Epoch 516/1000\n",
      " - 65s - loss: 0.0085 - val_loss: 0.0669\n",
      "Epoch 517/1000\n",
      " - 66s - loss: 0.0079 - val_loss: 0.0631\n",
      "Epoch 518/1000\n",
      " - 66s - loss: 0.0088 - val_loss: 0.0630\n",
      "Epoch 519/1000\n",
      " - 65s - loss: 0.0086 - val_loss: 0.0666\n",
      "Epoch 520/1000\n",
      " - 66s - loss: 0.0098 - val_loss: 0.0676\n",
      "Epoch 521/1000\n",
      " - 65s - loss: 0.0084 - val_loss: 0.0638\n",
      "Epoch 522/1000\n",
      " - 65s - loss: 0.0087 - val_loss: 0.0639\n",
      "Epoch 523/1000\n",
      " - 66s - loss: 0.0086 - val_loss: 0.0685\n",
      "Epoch 524/1000\n",
      " - 64s - loss: 0.0091 - val_loss: 0.0638\n",
      "Epoch 525/1000\n",
      " - 64s - loss: 0.0081 - val_loss: 0.0673\n",
      "Epoch 526/1000\n",
      " - 63s - loss: 0.0085 - val_loss: 0.0704\n",
      "Epoch 527/1000\n",
      " - 65s - loss: 0.0104 - val_loss: 0.0640\n",
      "Epoch 528/1000\n",
      " - 66s - loss: 0.0085 - val_loss: 0.0666\n",
      "Epoch 529/1000\n",
      " - 66s - loss: 0.0088 - val_loss: 0.0640\n",
      "Epoch 530/1000\n",
      " - 67s - loss: 0.0084 - val_loss: 0.0678\n",
      "Epoch 531/1000\n",
      " - 66s - loss: 0.0098 - val_loss: 0.0628\n",
      "Epoch 532/1000\n",
      " - 66s - loss: 0.0119 - val_loss: 0.0690\n",
      "Epoch 533/1000\n",
      " - 67s - loss: 0.0102 - val_loss: 0.0659\n",
      "Epoch 534/1000\n",
      " - 66s - loss: 0.0093 - val_loss: 0.0638\n",
      "Epoch 535/1000\n",
      " - 66s - loss: 0.0085 - val_loss: 0.0646\n",
      "Epoch 536/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0637\n",
      "Epoch 537/1000\n",
      " - 67s - loss: 0.0079 - val_loss: 0.0637\n",
      "Epoch 538/1000\n",
      " - 65s - loss: 0.0081 - val_loss: 0.0657\n",
      "Epoch 539/1000\n",
      " - 66s - loss: 0.0086 - val_loss: 0.0670\n",
      "Epoch 540/1000\n",
      " - 66s - loss: 0.0079 - val_loss: 0.0626\n",
      "Epoch 541/1000\n",
      " - 65s - loss: 0.0081 - val_loss: 0.0649\n",
      "Epoch 542/1000\n",
      " - 65s - loss: 0.0084 - val_loss: 0.0634\n",
      "Epoch 543/1000\n",
      " - 64s - loss: 0.0081 - val_loss: 0.0678\n",
      "Epoch 544/1000\n",
      " - 65s - loss: 0.0082 - val_loss: 0.0643\n",
      "Epoch 545/1000\n",
      " - 65s - loss: 0.0084 - val_loss: 0.0690\n",
      "Epoch 546/1000\n",
      " - 65s - loss: 0.0082 - val_loss: 0.0655\n",
      "Epoch 547/1000\n",
      " - 65s - loss: 0.0077 - val_loss: 0.0624\n",
      "Epoch 548/1000\n",
      " - 64s - loss: 0.0076 - val_loss: 0.0638\n",
      "Epoch 549/1000\n",
      " - 65s - loss: 0.0086 - val_loss: 0.0629\n",
      "Epoch 550/1000\n",
      " - 64s - loss: 0.0081 - val_loss: 0.0645\n",
      "Epoch 551/1000\n",
      " - 65s - loss: 0.0079 - val_loss: 0.0624\n",
      "Epoch 552/1000\n",
      " - 64s - loss: 0.0080 - val_loss: 0.0646\n",
      "Epoch 553/1000\n",
      " - 65s - loss: 0.0079 - val_loss: 0.0676\n",
      "Epoch 554/1000\n",
      " - 65s - loss: 0.0080 - val_loss: 0.0672\n",
      "Epoch 555/1000\n",
      " - 64s - loss: 0.0082 - val_loss: 0.0635\n",
      "Epoch 556/1000\n",
      " - 65s - loss: 0.0085 - val_loss: 0.0638\n",
      "Epoch 557/1000\n",
      " - 64s - loss: 0.0080 - val_loss: 0.0676\n",
      "Epoch 558/1000\n",
      " - 66s - loss: 0.0081 - val_loss: 0.0627\n",
      "Epoch 559/1000\n",
      " - 66s - loss: 0.0082 - val_loss: 0.0630\n",
      "Epoch 560/1000\n",
      " - 66s - loss: 0.0084 - val_loss: 0.0658\n",
      "Epoch 561/1000\n",
      " - 66s - loss: 0.0084 - val_loss: 0.0633\n",
      "Epoch 562/1000\n",
      " - 66s - loss: 0.0080 - val_loss: 0.0663\n",
      "Epoch 563/1000\n",
      " - 66s - loss: 0.0080 - val_loss: 0.0639\n",
      "Epoch 564/1000\n",
      " - 66s - loss: 0.0087 - val_loss: 0.0642\n",
      "Epoch 565/1000\n",
      " - 66s - loss: 0.0079 - val_loss: 0.0608\n",
      "Epoch 566/1000\n",
      " - 65s - loss: 0.0079 - val_loss: 0.0620\n",
      "Epoch 567/1000\n",
      " - 65s - loss: 0.0081 - val_loss: 0.0640\n",
      "Epoch 568/1000\n",
      " - 65s - loss: 0.0082 - val_loss: 0.0705\n",
      "Epoch 569/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0638\n",
      "Epoch 570/1000\n",
      " - 66s - loss: 0.0078 - val_loss: 0.0635\n",
      "Epoch 571/1000\n",
      " - 66s - loss: 0.0078 - val_loss: 0.0672\n",
      "Epoch 572/1000\n",
      " - 66s - loss: 0.0082 - val_loss: 0.0623\n",
      "Epoch 573/1000\n",
      " - 66s - loss: 0.0087 - val_loss: 0.0676\n",
      "Epoch 574/1000\n",
      " - 65s - loss: 0.0077 - val_loss: 0.0634\n",
      "Epoch 575/1000\n",
      " - 64s - loss: 0.0076 - val_loss: 0.0662\n",
      "Epoch 576/1000\n",
      " - 64s - loss: 0.0073 - val_loss: 0.0645\n",
      "Epoch 577/1000\n",
      " - 66s - loss: 0.0078 - val_loss: 0.0639\n",
      "Epoch 578/1000\n",
      " - 67s - loss: 0.0086 - val_loss: 0.0648\n",
      "Epoch 579/1000\n",
      " - 65s - loss: 0.0091 - val_loss: 0.0635\n",
      "Epoch 580/1000\n",
      " - 65s - loss: 0.0084 - val_loss: 0.0637\n",
      "Epoch 581/1000\n",
      " - 65s - loss: 0.0078 - val_loss: 0.0643\n",
      "Epoch 582/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0673\n",
      "Epoch 583/1000\n",
      " - 66s - loss: 0.0087 - val_loss: 0.0661\n",
      "Epoch 584/1000\n",
      " - 65s - loss: 0.0091 - val_loss: 0.0681\n",
      "Epoch 585/1000\n",
      " - 66s - loss: 0.0080 - val_loss: 0.0649\n",
      "Epoch 586/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0633\n",
      "Epoch 587/1000\n",
      " - 66s - loss: 0.0074 - val_loss: 0.0625\n",
      "Epoch 588/1000\n",
      " - 66s - loss: 0.0077 - val_loss: 0.0632\n",
      "Epoch 589/1000\n",
      " - 66s - loss: 0.0081 - val_loss: 0.0618\n",
      "Epoch 590/1000\n",
      " - 66s - loss: 0.0080 - val_loss: 0.0685\n",
      "Epoch 591/1000\n",
      " - 65s - loss: 0.0080 - val_loss: 0.0642\n",
      "Epoch 592/1000\n",
      " - 65s - loss: 0.0093 - val_loss: 0.0676\n",
      "Epoch 593/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0613\n",
      "Epoch 594/1000\n",
      " - 66s - loss: 0.0080 - val_loss: 0.0618\n",
      "Epoch 595/1000\n",
      " - 66s - loss: 0.0083 - val_loss: 0.0637\n",
      "Epoch 596/1000\n",
      " - 66s - loss: 0.0079 - val_loss: 0.0627\n",
      "Epoch 597/1000\n",
      " - 66s - loss: 0.0078 - val_loss: 0.0651\n",
      "Epoch 598/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0618\n",
      "Epoch 599/1000\n",
      " - 66s - loss: 0.0077 - val_loss: 0.0630\n",
      "Epoch 600/1000\n",
      " - 66s - loss: 0.0081 - val_loss: 0.0622\n",
      "Epoch 601/1000\n",
      " - 65s - loss: 0.0080 - val_loss: 0.0641\n",
      "Epoch 602/1000\n",
      " - 65s - loss: 0.0077 - val_loss: 0.0637\n",
      "Epoch 603/1000\n",
      " - 65s - loss: 0.0080 - val_loss: 0.0632\n",
      "Epoch 604/1000\n",
      " - 65s - loss: 0.0083 - val_loss: 0.0648\n",
      "Epoch 605/1000\n",
      " - 65s - loss: 0.0077 - val_loss: 0.0631\n",
      "Epoch 606/1000\n",
      " - 65s - loss: 0.0080 - val_loss: 0.0645\n",
      "Epoch 607/1000\n",
      " - 65s - loss: 0.0074 - val_loss: 0.0624\n",
      "Epoch 608/1000\n",
      " - 65s - loss: 0.0076 - val_loss: 0.0621\n",
      "Epoch 609/1000\n",
      " - 65s - loss: 0.0075 - val_loss: 0.0706\n",
      "Epoch 610/1000\n",
      " - 66s - loss: 0.0074 - val_loss: 0.0642\n",
      "Epoch 611/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0623\n",
      "Epoch 612/1000\n",
      " - 66s - loss: 0.0073 - val_loss: 0.0621\n",
      "Epoch 613/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0646\n",
      "Epoch 614/1000\n",
      " - 66s - loss: 0.0078 - val_loss: 0.0668\n",
      "Epoch 615/1000\n",
      " - 66s - loss: 0.0086 - val_loss: 0.0679\n",
      "Epoch 616/1000\n",
      " - 66s - loss: 0.0080 - val_loss: 0.0661\n",
      "Epoch 617/1000\n",
      " - 67s - loss: 0.0081 - val_loss: 0.0676\n",
      "Epoch 618/1000\n",
      " - 66s - loss: 0.0085 - val_loss: 0.0651\n",
      "Epoch 619/1000\n",
      " - 67s - loss: 0.0084 - val_loss: 0.0673\n",
      "Epoch 620/1000\n",
      " - 66s - loss: 0.0077 - val_loss: 0.0627\n",
      "Epoch 621/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0628\n",
      "Epoch 622/1000\n",
      " - 66s - loss: 0.0078 - val_loss: 0.0642\n",
      "Epoch 623/1000\n",
      " - 66s - loss: 0.0085 - val_loss: 0.0681\n",
      "Epoch 624/1000\n",
      " - 66s - loss: 0.0078 - val_loss: 0.0621\n",
      "Epoch 625/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0619\n",
      "Epoch 626/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0660\n",
      "Epoch 627/1000\n",
      " - 66s - loss: 0.0077 - val_loss: 0.0661\n",
      "Epoch 628/1000\n",
      " - 67s - loss: 0.0071 - val_loss: 0.0631\n",
      "Epoch 629/1000\n",
      " - 65s - loss: 0.0073 - val_loss: 0.0627\n",
      "Epoch 630/1000\n",
      " - 66s - loss: 0.0080 - val_loss: 0.0624\n",
      "Epoch 631/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0643\n",
      "Epoch 632/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0662\n",
      "Epoch 633/1000\n",
      " - 68s - loss: 0.0075 - val_loss: 0.0648\n",
      "Epoch 634/1000\n",
      " - 67s - loss: 0.0075 - val_loss: 0.0645\n",
      "Epoch 635/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0650\n",
      "Epoch 636/1000\n",
      " - 67s - loss: 0.0074 - val_loss: 0.0619\n",
      "Epoch 637/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0633\n",
      "Epoch 638/1000\n",
      " - 66s - loss: 0.0073 - val_loss: 0.0616\n",
      "Epoch 639/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0639\n",
      "Epoch 640/1000\n",
      " - 67s - loss: 0.0077 - val_loss: 0.0655\n",
      "Epoch 641/1000\n",
      " - 67s - loss: 0.0078 - val_loss: 0.0658\n",
      "Epoch 642/1000\n",
      " - 66s - loss: 0.0083 - val_loss: 0.0633\n",
      "Epoch 643/1000\n",
      " - 67s - loss: 0.0076 - val_loss: 0.0650\n",
      "Epoch 644/1000\n",
      " - 66s - loss: 0.0071 - val_loss: 0.0610\n",
      "Epoch 645/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0657\n",
      "Epoch 646/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0616\n",
      "Epoch 647/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0601\n",
      "Epoch 648/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0619\n",
      "Epoch 649/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0607\n",
      "Epoch 650/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0634\n",
      "Epoch 651/1000\n",
      " - 66s - loss: 0.0069 - val_loss: 0.0618\n",
      "Epoch 652/1000\n",
      " - 67s - loss: 0.0073 - val_loss: 0.0650\n",
      "Epoch 653/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0654\n",
      "Epoch 654/1000\n",
      " - 67s - loss: 0.0079 - val_loss: 0.0650\n",
      "Epoch 655/1000\n",
      " - 66s - loss: 0.0083 - val_loss: 0.0628\n",
      "Epoch 656/1000\n",
      " - 67s - loss: 0.0075 - val_loss: 0.0668\n",
      "Epoch 657/1000\n",
      " - 66s - loss: 0.0077 - val_loss: 0.0621\n",
      "Epoch 658/1000\n",
      " - 66s - loss: 0.0073 - val_loss: 0.0657\n",
      "Epoch 659/1000\n",
      " - 66s - loss: 0.0079 - val_loss: 0.0638\n",
      "Epoch 660/1000\n",
      " - 66s - loss: 0.0074 - val_loss: 0.0669\n",
      "Epoch 661/1000\n",
      " - 66s - loss: 0.0071 - val_loss: 0.0660\n",
      "Epoch 662/1000\n",
      " - 67s - loss: 0.0072 - val_loss: 0.0629\n",
      "Epoch 663/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0619\n",
      "Epoch 664/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0611\n",
      "Epoch 665/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0662\n",
      "Epoch 666/1000\n",
      " - 66s - loss: 0.0071 - val_loss: 0.0645\n",
      "Epoch 667/1000\n",
      " - 67s - loss: 0.0072 - val_loss: 0.0683\n",
      "Epoch 668/1000\n",
      " - 66s - loss: 0.0078 - val_loss: 0.0626\n",
      "Epoch 669/1000\n",
      " - 66s - loss: 0.0068 - val_loss: 0.0645\n",
      "Epoch 670/1000\n",
      " - 66s - loss: 0.0067 - val_loss: 0.0634\n",
      "Epoch 671/1000\n",
      " - 66s - loss: 0.0067 - val_loss: 0.0621\n",
      "Epoch 672/1000\n",
      " - 66s - loss: 0.0070 - val_loss: 0.0633\n",
      "Epoch 673/1000\n",
      " - 66s - loss: 0.0070 - val_loss: 0.0645\n",
      "Epoch 674/1000\n",
      " - 66s - loss: 0.0074 - val_loss: 0.0640\n",
      "Epoch 675/1000\n",
      " - 66s - loss: 0.0066 - val_loss: 0.0618\n",
      "Epoch 676/1000\n",
      " - 66s - loss: 0.0070 - val_loss: 0.0632\n",
      "Epoch 677/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0665\n",
      "Epoch 678/1000\n",
      " - 66s - loss: 0.0071 - val_loss: 0.0632\n",
      "Epoch 679/1000\n",
      " - 68s - loss: 0.0072 - val_loss: 0.0639\n",
      "Epoch 680/1000\n",
      " - 68s - loss: 0.0074 - val_loss: 0.0721\n",
      "Epoch 681/1000\n",
      " - 67s - loss: 0.0076 - val_loss: 0.0643\n",
      "Epoch 682/1000\n",
      " - 66s - loss: 0.0075 - val_loss: 0.0631\n",
      "Epoch 683/1000\n",
      " - 66s - loss: 0.0071 - val_loss: 0.0630\n",
      "Epoch 684/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0669\n",
      "Epoch 685/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0678\n",
      "Epoch 686/1000\n",
      " - 67s - loss: 0.0069 - val_loss: 0.0650\n",
      "Epoch 687/1000\n",
      " - 66s - loss: 0.0071 - val_loss: 0.0621\n",
      "Epoch 688/1000\n",
      " - 66s - loss: 0.0067 - val_loss: 0.0620\n",
      "Epoch 689/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0651\n",
      "Epoch 690/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0645\n",
      "Epoch 691/1000\n",
      " - 66s - loss: 0.0067 - val_loss: 0.0640\n",
      "Epoch 692/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0631\n",
      "Epoch 693/1000\n",
      " - 67s - loss: 0.0078 - val_loss: 0.0637\n",
      "Epoch 694/1000\n",
      " - 67s - loss: 0.0072 - val_loss: 0.0610\n",
      "Epoch 695/1000\n",
      " - 66s - loss: 0.0070 - val_loss: 0.0621\n",
      "Epoch 696/1000\n",
      " - 67s - loss: 0.0073 - val_loss: 0.0632\n",
      "Epoch 697/1000\n",
      " - 66s - loss: 0.0074 - val_loss: 0.0630\n",
      "Epoch 698/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0628\n",
      "Epoch 699/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0617\n",
      "Epoch 700/1000\n",
      " - 67s - loss: 0.0069 - val_loss: 0.0608\n",
      "Epoch 701/1000\n",
      " - 67s - loss: 0.0080 - val_loss: 0.0627\n",
      "Epoch 702/1000\n",
      " - 67s - loss: 0.0073 - val_loss: 0.0629\n",
      "Epoch 703/1000\n",
      " - 67s - loss: 0.0076 - val_loss: 0.0622\n",
      "Epoch 704/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0633\n",
      "Epoch 705/1000\n",
      " - 67s - loss: 0.0073 - val_loss: 0.0605\n",
      "Epoch 706/1000\n",
      " - 67s - loss: 0.0073 - val_loss: 0.0627\n",
      "Epoch 707/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0631\n",
      "Epoch 708/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0639\n",
      "Epoch 709/1000\n",
      " - 67s - loss: 0.0068 - val_loss: 0.0632\n",
      "Epoch 710/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0602\n",
      "Epoch 711/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0642\n",
      "Epoch 712/1000\n",
      " - 66s - loss: 0.0066 - val_loss: 0.0657\n",
      "Epoch 713/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0625\n",
      "Epoch 714/1000\n",
      " - 67s - loss: 0.0069 - val_loss: 0.0619\n",
      "Epoch 715/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0645\n",
      "Epoch 716/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0635\n",
      "Epoch 717/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0638\n",
      "Epoch 718/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0627\n",
      "Epoch 719/1000\n",
      " - 66s - loss: 0.0079 - val_loss: 0.0639\n",
      "Epoch 720/1000\n",
      " - 67s - loss: 0.0071 - val_loss: 0.0622\n",
      "Epoch 721/1000\n",
      " - 67s - loss: 0.0073 - val_loss: 0.0668\n",
      "Epoch 722/1000\n",
      " - 67s - loss: 0.0072 - val_loss: 0.0661\n",
      "Epoch 723/1000\n",
      " - 66s - loss: 0.0070 - val_loss: 0.0655\n",
      "Epoch 724/1000\n",
      " - 66s - loss: 0.0068 - val_loss: 0.0657\n",
      "Epoch 725/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0650\n",
      "Epoch 726/1000\n",
      " - 65s - loss: 0.0068 - val_loss: 0.0651\n",
      "Epoch 727/1000\n",
      " - 65s - loss: 0.0065 - val_loss: 0.0636\n",
      "Epoch 728/1000\n",
      " - 66s - loss: 0.0068 - val_loss: 0.0654\n",
      "Epoch 729/1000\n",
      " - 67s - loss: 0.0069 - val_loss: 0.0640\n",
      "Epoch 730/1000\n",
      " - 68s - loss: 0.0068 - val_loss: 0.0633\n",
      "Epoch 731/1000\n",
      " - 67s - loss: 0.0072 - val_loss: 0.0636\n",
      "Epoch 732/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0643\n",
      "Epoch 733/1000\n",
      " - 67s - loss: 0.0065 - val_loss: 0.0620\n",
      "Epoch 734/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0630\n",
      "Epoch 735/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0653\n",
      "Epoch 736/1000\n",
      " - 67s - loss: 0.0074 - val_loss: 0.0617\n",
      "Epoch 737/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0613\n",
      "Epoch 738/1000\n",
      " - 67s - loss: 0.0063 - val_loss: 0.0635\n",
      "Epoch 739/1000\n",
      " - 66s - loss: 0.0065 - val_loss: 0.0646\n",
      "Epoch 740/1000\n",
      " - 66s - loss: 0.0069 - val_loss: 0.0625\n",
      "Epoch 741/1000\n",
      " - 67s - loss: 0.0069 - val_loss: 0.0660\n",
      "Epoch 742/1000\n",
      " - 66s - loss: 0.0066 - val_loss: 0.0631\n",
      "Epoch 743/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0636\n",
      "Epoch 744/1000\n",
      " - 67s - loss: 0.0068 - val_loss: 0.0627\n",
      "Epoch 745/1000\n",
      " - 66s - loss: 0.0069 - val_loss: 0.0631\n",
      "Epoch 746/1000\n",
      " - 67s - loss: 0.0071 - val_loss: 0.0624\n",
      "Epoch 747/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0653\n",
      "Epoch 748/1000\n",
      " - 67s - loss: 0.0068 - val_loss: 0.0622\n",
      "Epoch 749/1000\n",
      " - 66s - loss: 0.0066 - val_loss: 0.0637\n",
      "Epoch 750/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0621\n",
      "Epoch 751/1000\n",
      " - 67s - loss: 0.0065 - val_loss: 0.0639\n",
      "Epoch 752/1000\n",
      " - 66s - loss: 0.0068 - val_loss: 0.0633\n",
      "Epoch 753/1000\n",
      " - 67s - loss: 0.0069 - val_loss: 0.0654\n",
      "Epoch 754/1000\n",
      " - 66s - loss: 0.0082 - val_loss: 0.0622\n",
      "Epoch 755/1000\n",
      " - 66s - loss: 0.0064 - val_loss: 0.0608\n",
      "Epoch 756/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0655\n",
      "Epoch 757/1000\n",
      " - 66s - loss: 0.0076 - val_loss: 0.0619\n",
      "Epoch 758/1000\n",
      " - 66s - loss: 0.0074 - val_loss: 0.0637\n",
      "Epoch 759/1000\n",
      " - 66s - loss: 0.0069 - val_loss: 0.0634\n",
      "Epoch 760/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0656\n",
      "Epoch 761/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0639\n",
      "Epoch 762/1000\n",
      " - 66s - loss: 0.0062 - val_loss: 0.0630\n",
      "Epoch 763/1000\n",
      " - 67s - loss: 0.0063 - val_loss: 0.0607\n",
      "Epoch 764/1000\n",
      " - 66s - loss: 0.0062 - val_loss: 0.0660\n",
      "Epoch 765/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0609\n",
      "Epoch 766/1000\n",
      " - 66s - loss: 0.0062 - val_loss: 0.0622\n",
      "Epoch 767/1000\n",
      " - 66s - loss: 0.0062 - val_loss: 0.0603\n",
      "Epoch 768/1000\n",
      " - 67s - loss: 0.0070 - val_loss: 0.0621\n",
      "Epoch 769/1000\n",
      " - 66s - loss: 0.0070 - val_loss: 0.0609\n",
      "Epoch 770/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0612\n",
      "Epoch 771/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0711\n",
      "Epoch 772/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0632\n",
      "Epoch 773/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0652\n",
      "Epoch 774/1000\n",
      " - 66s - loss: 0.0064 - val_loss: 0.0616\n",
      "Epoch 775/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0634\n",
      "Epoch 776/1000\n",
      " - 66s - loss: 0.0062 - val_loss: 0.0658\n",
      "Epoch 777/1000\n",
      " - 65s - loss: 0.0068 - val_loss: 0.0637\n",
      "Epoch 778/1000\n",
      " - 65s - loss: 0.0067 - val_loss: 0.0644\n",
      "Epoch 779/1000\n",
      " - 68s - loss: 0.0063 - val_loss: 0.0634\n",
      "Epoch 780/1000\n",
      " - 68s - loss: 0.0063 - val_loss: 0.0631\n",
      "Epoch 781/1000\n",
      " - 67s - loss: 0.0058 - val_loss: 0.0595\n",
      "Epoch 782/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0625\n",
      "Epoch 783/1000\n",
      " - 67s - loss: 0.0075 - val_loss: 0.0629\n",
      "Epoch 784/1000\n",
      " - 67s - loss: 0.0068 - val_loss: 0.0634\n",
      "Epoch 785/1000\n",
      " - 67s - loss: 0.0061 - val_loss: 0.0615\n",
      "Epoch 786/1000\n",
      " - 66s - loss: 0.0065 - val_loss: 0.0615\n",
      "Epoch 787/1000\n",
      " - 67s - loss: 0.0063 - val_loss: 0.0641\n",
      "Epoch 788/1000\n",
      " - 66s - loss: 0.0061 - val_loss: 0.0649\n",
      "Epoch 789/1000\n",
      " - 67s - loss: 0.0074 - val_loss: 0.0644\n",
      "Epoch 790/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0633\n",
      "Epoch 791/1000\n",
      " - 66s - loss: 0.0063 - val_loss: 0.0655\n",
      "Epoch 792/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0640\n",
      "Epoch 793/1000\n",
      " - 66s - loss: 0.0072 - val_loss: 0.0630\n",
      "Epoch 794/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0597\n",
      "Epoch 795/1000\n",
      " - 68s - loss: 0.0061 - val_loss: 0.0606\n",
      "Epoch 796/1000\n",
      " - 67s - loss: 0.0065 - val_loss: 0.0619\n",
      "Epoch 797/1000\n",
      " - 68s - loss: 0.0067 - val_loss: 0.0619\n",
      "Epoch 798/1000\n",
      " - 67s - loss: 0.0068 - val_loss: 0.0635\n",
      "Epoch 799/1000\n",
      " - 68s - loss: 0.0063 - val_loss: 0.0625\n",
      "Epoch 800/1000\n",
      " - 68s - loss: 0.0065 - val_loss: 0.0630\n",
      "Epoch 801/1000\n",
      " - 67s - loss: 0.0060 - val_loss: 0.0648\n",
      "Epoch 802/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0632\n",
      "Epoch 803/1000\n",
      " - 68s - loss: 0.0061 - val_loss: 0.0604\n",
      "Epoch 804/1000\n",
      " - 68s - loss: 0.0059 - val_loss: 0.0614\n",
      "Epoch 805/1000\n",
      " - 68s - loss: 0.0063 - val_loss: 0.0633\n",
      "Epoch 806/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0645\n",
      "Epoch 807/1000\n",
      " - 68s - loss: 0.0065 - val_loss: 0.0663\n",
      "Epoch 808/1000\n",
      " - 68s - loss: 0.0069 - val_loss: 0.0632\n",
      "Epoch 809/1000\n",
      " - 68s - loss: 0.0061 - val_loss: 0.0616\n",
      "Epoch 810/1000\n",
      " - 68s - loss: 0.0068 - val_loss: 0.0634\n",
      "Epoch 811/1000\n",
      " - 68s - loss: 0.0060 - val_loss: 0.0613\n",
      "Epoch 812/1000\n",
      " - 68s - loss: 0.0062 - val_loss: 0.0627\n",
      "Epoch 813/1000\n",
      " - 68s - loss: 0.0063 - val_loss: 0.0614\n",
      "Epoch 814/1000\n",
      " - 68s - loss: 0.0063 - val_loss: 0.0638\n",
      "Epoch 815/1000\n",
      " - 68s - loss: 0.0065 - val_loss: 0.0621\n",
      "Epoch 816/1000\n",
      " - 67s - loss: 0.0063 - val_loss: 0.0607\n",
      "Epoch 817/1000\n",
      " - 68s - loss: 0.0062 - val_loss: 0.0613\n",
      "Epoch 818/1000\n",
      " - 68s - loss: 0.0071 - val_loss: 0.0629\n",
      "Epoch 819/1000\n",
      " - 68s - loss: 0.0070 - val_loss: 0.0632\n",
      "Epoch 820/1000\n",
      " - 68s - loss: 0.0065 - val_loss: 0.0620\n",
      "Epoch 821/1000\n",
      " - 66s - loss: 0.0070 - val_loss: 0.0629\n",
      "Epoch 822/1000\n",
      " - 66s - loss: 0.0062 - val_loss: 0.0671\n",
      "Epoch 823/1000\n",
      " - 66s - loss: 0.0066 - val_loss: 0.0642\n",
      "Epoch 824/1000\n",
      " - 68s - loss: 0.0062 - val_loss: 0.0630\n",
      "Epoch 825/1000\n",
      " - 68s - loss: 0.0064 - val_loss: 0.0628\n",
      "Epoch 826/1000\n",
      " - 68s - loss: 0.0063 - val_loss: 0.0625\n",
      "Epoch 827/1000\n",
      " - 68s - loss: 0.0062 - val_loss: 0.0626\n",
      "Epoch 828/1000\n",
      " - 68s - loss: 0.0064 - val_loss: 0.0723\n",
      "Epoch 829/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0612\n",
      "Epoch 830/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0611\n",
      "Epoch 831/1000\n",
      " - 67s - loss: 0.0061 - val_loss: 0.0625\n",
      "Epoch 832/1000\n",
      " - 67s - loss: 0.0063 - val_loss: 0.0639\n",
      "Epoch 833/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0614\n",
      "Epoch 834/1000\n",
      " - 66s - loss: 0.0058 - val_loss: 0.0665\n",
      "Epoch 835/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0610\n",
      "Epoch 836/1000\n",
      " - 67s - loss: 0.0057 - val_loss: 0.0631\n",
      "Epoch 837/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0649\n",
      "Epoch 838/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0645\n",
      "Epoch 839/1000\n",
      " - 67s - loss: 0.0059 - val_loss: 0.0629\n",
      "Epoch 840/1000\n",
      " - 67s - loss: 0.0058 - val_loss: 0.0621\n",
      "Epoch 841/1000\n",
      " - 67s - loss: 0.0058 - val_loss: 0.0600\n",
      "Epoch 842/1000\n",
      " - 67s - loss: 0.0058 - val_loss: 0.0652\n",
      "Epoch 843/1000\n",
      " - 67s - loss: 0.0063 - val_loss: 0.0677\n",
      "Epoch 844/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0609\n",
      "Epoch 845/1000\n",
      " - 67s - loss: 0.0065 - val_loss: 0.0619\n",
      "Epoch 846/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0620\n",
      "Epoch 847/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0644\n",
      "Epoch 848/1000\n",
      " - 67s - loss: 0.0061 - val_loss: 0.0600\n",
      "Epoch 849/1000\n",
      " - 66s - loss: 0.0057 - val_loss: 0.0627\n",
      "Epoch 850/1000\n",
      " - 66s - loss: 0.0062 - val_loss: 0.0614\n",
      "Epoch 851/1000\n",
      " - 67s - loss: 0.0058 - val_loss: 0.0632\n",
      "Epoch 852/1000\n",
      " - 67s - loss: 0.0064 - val_loss: 0.0643\n",
      "Epoch 853/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0611\n",
      "Epoch 854/1000\n",
      " - 67s - loss: 0.0060 - val_loss: 0.0634\n",
      "Epoch 855/1000\n",
      " - 67s - loss: 0.0061 - val_loss: 0.0668\n",
      "Epoch 856/1000\n",
      " - 66s - loss: 0.0065 - val_loss: 0.0630\n",
      "Epoch 857/1000\n",
      " - 67s - loss: 0.0067 - val_loss: 0.0649\n",
      "Epoch 858/1000\n",
      " - 68s - loss: 0.0065 - val_loss: 0.0679\n",
      "Epoch 859/1000\n",
      " - 68s - loss: 0.0064 - val_loss: 0.0635\n",
      "Epoch 860/1000\n",
      " - 68s - loss: 0.0065 - val_loss: 0.0611\n",
      "Epoch 861/1000\n",
      " - 68s - loss: 0.0061 - val_loss: 0.0618\n",
      "Epoch 862/1000\n",
      " - 68s - loss: 0.0061 - val_loss: 0.0620\n",
      "Epoch 863/1000\n",
      " - 68s - loss: 0.0056 - val_loss: 0.0619\n",
      "Epoch 864/1000\n",
      " - 67s - loss: 0.0060 - val_loss: 0.0627\n",
      "Epoch 865/1000\n",
      " - 67s - loss: 0.0065 - val_loss: 0.0632\n",
      "Epoch 866/1000\n",
      " - 68s - loss: 0.0058 - val_loss: 0.0608\n",
      "Epoch 867/1000\n",
      " - 66s - loss: 0.0059 - val_loss: 0.0616\n",
      "Epoch 868/1000\n",
      " - 66s - loss: 0.0063 - val_loss: 0.0641\n",
      "Epoch 869/1000\n",
      " - 66s - loss: 0.0059 - val_loss: 0.0618\n",
      "Epoch 870/1000\n",
      " - 69s - loss: 0.0063 - val_loss: 0.0625\n",
      "Epoch 871/1000\n",
      " - 69s - loss: 0.0062 - val_loss: 0.0605\n",
      "Epoch 872/1000\n",
      " - 68s - loss: 0.0056 - val_loss: 0.0624\n",
      "Epoch 873/1000\n",
      " - 68s - loss: 0.0060 - val_loss: 0.0615\n",
      "Epoch 874/1000\n",
      " - 68s - loss: 0.0056 - val_loss: 0.0619\n",
      "Epoch 875/1000\n",
      " - 67s - loss: 0.0056 - val_loss: 0.0616\n",
      "Epoch 876/1000\n",
      " - 67s - loss: 0.0061 - val_loss: 0.0645\n",
      "Epoch 877/1000\n",
      " - 67s - loss: 0.0059 - val_loss: 0.0619\n",
      "Epoch 878/1000\n",
      " - 68s - loss: 0.0056 - val_loss: 0.0632\n",
      "Epoch 879/1000\n",
      " - 67s - loss: 0.0062 - val_loss: 0.0612\n",
      "Epoch 880/1000\n",
      " - 67s - loss: 0.0060 - val_loss: 0.0602\n",
      "Epoch 881/1000\n",
      " - 67s - loss: 0.0061 - val_loss: 0.0626\n",
      "Epoch 882/1000\n",
      " - 67s - loss: 0.0066 - val_loss: 0.0614\n",
      "Epoch 883/1000\n",
      " - 67s - loss: 0.0060 - val_loss: 0.0617\n",
      "Epoch 884/1000\n",
      " - 67s - loss: 0.0063 - val_loss: 0.0623\n",
      "Epoch 885/1000\n",
      " - 67s - loss: 0.0063 - val_loss: 0.0654\n",
      "Epoch 886/1000\n",
      " - 68s - loss: 0.0057 - val_loss: 0.0629\n",
      "Epoch 887/1000\n",
      " - 68s - loss: 0.0053 - val_loss: 0.0613\n",
      "Epoch 888/1000\n",
      " - 67s - loss: 0.0060 - val_loss: 0.0653\n",
      "Epoch 889/1000\n",
      " - 68s - loss: 0.0081 - val_loss: 0.0603\n",
      "Epoch 890/1000\n",
      " - 67s - loss: 0.0058 - val_loss: 0.0614\n",
      "Epoch 891/1000\n",
      " - 67s - loss: 0.0059 - val_loss: 0.0600\n",
      "Epoch 892/1000\n",
      " - 68s - loss: 0.0065 - val_loss: 0.0620\n",
      "Epoch 893/1000\n",
      " - 68s - loss: 0.0060 - val_loss: 0.0613\n",
      "Epoch 894/1000\n",
      " - 67s - loss: 0.0059 - val_loss: 0.0616\n",
      "Epoch 895/1000\n",
      " - 68s - loss: 0.0057 - val_loss: 0.0621\n",
      "Epoch 896/1000\n",
      " - 68s - loss: 0.0059 - val_loss: 0.0611\n",
      "Epoch 897/1000\n",
      " - 68s - loss: 0.0055 - val_loss: 0.0643\n",
      "Epoch 898/1000\n"
     ]
    }
   ],
   "source": [
    "dense_unet.fit(train_X_n, train_Y_n, batch_size=batch_size, epochs=epochs,verbose=2,validation_split=0.2) # directional vectors, without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1264, 8, 8, 8, 288)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1264, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
