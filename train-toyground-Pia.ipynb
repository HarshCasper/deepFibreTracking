{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from agent_pia import Agent, Action_Scheduler\n",
    "\n",
    "import os, sys\n",
    "sys.path.append('ext/deepFibreTracking/')\n",
    "\n",
    "import envs.RLtractEnvironment as RLTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3000000\n",
    "replay_memory_size = 5000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 20000\n",
    "eval_runs = 5\n",
    "network_update_every = 600\n",
    "\n",
    "max_episode_length = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed streamlines (data/HCP307200_DTI_smallSet.vtk) for ID 100307\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "env = RLTe.RLtractEnvironment(device = 'cpu')\n",
    "n_actions = env.action_space.n\n",
    "#print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init agent\n",
      "Init epsilon-greedy action scheduler\n",
      "Start training...\n",
      "[10] 911, -14.28779296875\n",
      "[20] 1940, -9.740166473388673\n",
      "[30] 3085, -33.659226735432945\n",
      "[40] 3713, -26.118430137634277\n",
      "[50] 4371, -25.328152923583986\n",
      "[60] 5615, -30.965625381469728\n",
      "[70] 6676, -31.822925567626953\n",
      "[80] 7411, -31.473579978942873\n",
      "[90] 8282, -28.919224124484593\n",
      "[100] 9245, -27.33523506164551\n",
      "[110] 10195, -27.400820541381837\n",
      "[120] 11348, -29.401457481384277\n",
      "[130] 12369, -27.028187217712404\n",
      "[140] 13048, -28.239447441101074\n"
     ]
    }
   ],
   "source": [
    "print(\"Init agent\")\n",
    "state = env.reset()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.getValue().shape[0], device=device, hidden=256, agent_history_length=agent_history_length, memory_size=replay_memory_size)\n",
    "\n",
    "print(\"Init epsilon-greedy action scheduler\")\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, replay_memory_start_size=replay_memory_size, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "    \n",
    "rewards = []\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "\n",
    "    ######## fill memory begins here\n",
    "    while epoch_step < evaluate_every:  # To Do implement evaluation\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        \n",
    "        #fill replay memory while interacting with env\n",
    "        for _ in range(max_episode_length):\n",
    "            # get action with epsilon-greedy strategy\n",
    "            try:\n",
    "                action = action_scheduler.get_action(step_counter, state.getValue().unsqueeze(0))\n",
    "            except PointOutsideOfDWIError:\n",
    "                action = n_actions-1\n",
    "\n",
    "            # perform step on environment\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            \n",
    "            # increase counter\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "            \n",
    "\n",
    "            # add current state, action, reward and terminal flag to memory\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                               state=state,\n",
    "                                               reward=reward,\n",
    "                                               terminal=terminal)\n",
    "            \n",
    "            # prepare for next step\n",
    "            state = next_state\n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > replay_memory_size:\n",
    "                loss = agent.optimize()\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > replay_memory_size and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if episode ended before maximum step\n",
    "            if terminal:\n",
    "                #print(\"terminal reached\")\n",
    "                terminal = False\n",
    "                state = env.reset()\n",
    "                episode_reward_sum = 0\n",
    "                break\n",
    "        #print(\"Append reward\")\n",
    "        rewards.append(episode_reward_sum)\n",
    "\n",
    "        if len(rewards) % 10 == 0:\n",
    "            print(\"[{}] {}, {}\".format(len(rewards), step_counter, np.mean(rewards[-100:])))\n",
    "\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        eval_episode_reward = 0\n",
    "        while eval_steps < max_episode_length:\n",
    "            action = action_scheduler.get_action(step_counter, state.getValue().unsqueeze(0), evaluation=True)\n",
    "\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            eval_steps += 1\n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "    \n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p 'checkpoints/'\n",
    "#torch.save(agent.main_dqn.state_dict(), 'checkpoints/fiber_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(rewards[-100:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA (atari)",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
